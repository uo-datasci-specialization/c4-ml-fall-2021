---
title: K-Nearest Neighbors
subtitle: Applied Machine Learning for Educational Data Science
author:
  name: Cengiz Zopluoglu
  affiliation: University of Oregon | EDLD 654
date: 11/01/2021 ## Or "Lecture no."
output: 
  html_document:
    keep_md: false
    theme: journal
    highlight: haddock
    code_folding: show
    toc: yes
    toc_depth: 4
    toc_float: yes
  pdf_document:
    extra_dependencies: ["amssymb","animate","upgreek","amsmath"]
    keep_tex: false ## Change to true if want keep intermediate .tex file
    toc: true
    toc_depth: 3
    dev: cairo_pdf
always_allow_html: true
urlcolor: blue
mainfont: cochineal
sansfont: Fira Sans
monofont: Fira Code ## Although, see: https://tex.stackexchange.com/q/294362

## Automatically knit to both formats:
knit: (function(inputFile, encoding) {
 rmarkdown::render(inputFile, encoding = encoding, 
 output_format = 'all') 
 })
---

```{r klippy, echo=FALSE, include=TRUE}
klippy::klippy(position=c('top','right'))
```

`r paste('[Updated:',format(Sys.time(),'%a, %b %d, %Y - %H:%M:%S'),']')`

<style>
  
  .list-group-item.active, .list-group-item.active:focus, .list-group-item.active:hover {
    z-index: 2;
    color: #fff;
      background-color: #FC4445;
      border-color: #97CAEF;
  }

#infobox {
padding: 1em 1em 1em 4em;
margin-bottom: 10px;
border: 2px solid black;
border-radius: 10px;
background: #E6F6DC 5px center/3em no-repeat;
  }

</style>
  
  
```{r setup, include=FALSE}
knitr::opts_chunk$set(comment = "",fig.align='center')
require(here)
require(ggplot2)
require(plot3D)
require(kableExtra)
require(knitr)
require(giski)
require(magick)
require(gridExtra)
require(plotly)
options(scipen=99)


# Resources:
# https://bradleyboehmke.github.io/HOML/knn.html
# Applied Predictive Modeling, Chapter 13.5
# https://towardsdatascience.com/machine-learning-basics-with-the-k-nearest-neighbors-algorithm-6a6e71d01761
#https://towardsdatascience.com/a-simple-introduction-to-k-nearest-neighbors-algorithm-b3519ed98e
```

# 1. Distance Between Two Vectors

Measuring distance between two data points is at the core of K Nearest Neighbors (KNN) algorithm, and it is important to understand the concept of distance between two vectors. 

Imagine that each observation in a dataset lives in an *P*-dimensional space, where *P* is the number of predictors. 

- Obsevation 1: $\mathbf{A} = (A_1, A_2, A_3, ..., A_P)$
- Obsevation 2: $\mathbf{B} = (B_1, B_2, B_3, ..., B_P)$

A general definition of distance between two vectors is the **Minkowski Distance**. The Minkowski Distance can be defined as 

$$\left ( \sum_{i=1}^{P}|A_i-B_i|^q \right )^{\frac{1}{q}},$$
where $q$ can take any positive value.

For the sake of simplicity, suppose that we have two observations and three predictors, and we observe the following values for the two observations on these three predictors.

- Observation 1:  (20,25,30)

- Observation 2:  (80,90,75)

```{r, echo = FALSE, eval=TRUE, warning=FALSE}

x1 <- c(20,80)
x2 <- c(25,90)
x3 <- c(30,75)

plot_ly(x = x1, y = x2, z = x3,type='scatter3d',mode='markers',
        width=800,height=500) %>%
  layout(scene = list(xaxis = list(range = c(0,100),title='X1'),
                      yaxis = list(range = c(0,100),title='X2'),
                      zaxis = list(range = c(0,100),title='X3'))) %>%
  layout(scene = list(camera = list(eye = list(x = 1.25,y = 1.25,z = 0))))
```


If we assume that the $q=1$ for the Minkowski equation above, then the we can calculate the distance as the following:

```{r, echo = TRUE, eval=TRUE, warning=FALSE}

A <- c(20,25,30)
B <- c(80,90,75)


sum(abs(A - B))


```

When $q$ is equal to 1 for the Minkowski equation, it becomes a special case and is known as **Manhattan Distance**. Manhattan Distance between these two data points is visualized below.

```{r, echo = FALSE, eval=TRUE,warning=FALSE}


plot_ly(x = x1, y = x2, z = x3,type='scatter3d',mode='markers',
        width=800,height=500) %>%
  layout(scene = list(xaxis = list(range = c(0,100),title='X1'),
                      yaxis = list(range = c(0,100),title='X2'),
                      zaxis = list(range = c(0,100),title='X3'),
                      camera= list(eye = list(x = 1.25,y=1.25,z=1.25)))) %>%
  add_trace(x =c(x1[1],x1[1]),y=c(x2[1],x2[1]),z=c(x3[1],x3[2]), mode="lines") %>%
  add_trace(x =c(x1[1],x1[1]),y=c(x2[1],x2[2]),z=c(x3[2],x3[2]), mode="lines") %>%
  add_trace(x =c(x1[1],x1[2]),y=c(x2[2],x2[2]),z=c(x3[2],x3[2]), mode="lines") %>%
  layout(showlegend = FALSE) %>%
  layout(scene = list(camera = list(eye = list(x = 1.25,y = 1.25,z = 0))))

```

If we assume that the $q=2$ for the Minkowski equation above, then the we can calculate the distance as the following:

```{r, echo = TRUE, eval=TRUE, warning=FALSE}

A <- c(20,25,30)
B <- c(80,90,75)


(sum(abs(A - B)^2))^(1/2)


```

When $q$ is equal to 2 for the Minkowski equation, it is also a special case and is known as **Euclidian Distance**. Euclidian Distance between these two data points is visualized below.

```{r, echo = FALSE, eval=TRUE,warning=FALSE}

plot_ly(x = x1, y = x2, z = x3,type='scatter3d',mode='markers',
        width=800,height=500) %>%
  layout(scene = list(xaxis = list(range = c(0,100),title='X1'),
                      yaxis = list(range = c(0,100),title='X2'),
                      zaxis = list(range = c(0,100),title='X3'))) %>%
  add_trace(x =x1,y=x2,z=x3,mode='lines') %>%
  layout(showlegend = FALSE) %>%
  layout(scene = list(camera = list(eye = list(x = 1.25,y = 1,z = 0))))

```

# 2. K-Nearest Neighbors

Given $N$ observations in a dataset, a distance between any observation and $N-1$ remaining observations using Minkowski distance (with a user-defined choice of $q$ value). Then, for any given observation, we can rank order the remaining observations based on how close they are to the given observation and then decide the K nearest neighbors ($K = 1, 2, 3, ..., N-1$), K observations closest to the given observation based on their distance.

Suppose that there are 10 observations measured on three predictor variables (X1, X2, and X3) with the following values. 

```{r, echo = TRUE, eval=TRUE,warning=FALSE}

d <- data.frame(x1 =c(20,25,30,42,10,60,65,55,80,90),
                x2 =c(10,15,12,20,45,75,70,80,85,90),
                x3 =c(25,30,35,20,40,80,85,90,92,95),
                label= c('A','B','C','D','E','F','G','H','I','J'))

d

```

```{r, echo = FALSE, eval=TRUE,warning=FALSE}

x1 <- c(20,25,30,42,10)
x2 <- c(10,15,12,20,45)
x3 <- c(25,30,35,20,40)

y1 <- c(60,65,55,80,90)
y2 <- c(75,70,80,85,90)
y3 <- c(80,85,90,92,95)

plot_ly(x = x1, y = x2, z = x3,type='scatter3d',mode='markers',
        width=800,height=500) %>%
  layout(scene = list(xaxis = list(range = c(0,100),title='X1'),
                      yaxis = list(range = c(0,100),title='X2'),
                      zaxis = list(range = c(0,100),title='X3'))) %>%
  add_trace(x = y1, y = y2, z = y3,mode='markers',marker = list(color='orange')) %>%
  layout(showlegend = FALSE) %>%
  add_text(x = x1, y = x2, z = x3,text=c('A','B','C','D','E')) %>%
  add_text(x = y1, y = y2, z = y3,text=c('F','G','H','I','J')) %>%
  layout(scene = list(camera = list(eye = list(x = 1.5,y = 1,z = 1))))
```

Given that there 10 observations, we can calculate the distance between all 45 pairs of observations (e.g., Euclidian distance). 

```{r, echo = TRUE, eval=TRUE,warning=FALSE}
dist <- as.data.frame(t(combn(1:10,2)))
dist$euclidian <- NA

for(i in 1:nrow(dist)){
  
  a <- d[dist[i,1],1:3]
  b <- d[dist[i,2],1:3]
  dist[i,]$euclidian <- sqrt(sum((a-b)^2))
  
}

dist
```

Now, for instance, we can find the three closest observation to **Point E** (3-Nearest Neighbors). As seen below, the 3-Nearest Neighbors for **Point E** in this dataset would be **Point B**, **Point C**, and **Point A**.

```{r, echo = TRUE, eval=TRUE,warning=FALSE}

# Point E is the fifth observation in the dataset

loc <- which(dist[,1]==5 | dist[,2]==5)

tmp <- dist[loc,]

tmp[order(tmp$euclidian),]

```

***
<div id="infobox">

<center style="color:black;"> **NOTE 1** </center>

The $q$ in the Minkowski distance equation and $K$ in the K-nearest neighbor are user-defined hyperparameters in the KNN algorithm. As a researcher and model builder, you can pick any values for $q$ and $K$. They can be tuned using a similar approach applied in earlier classes for regularized regression models. One can pick a set of values for these hyperparameters and apply a grid search to find the combination that provides the best predictive performance.

It is typical to observe overfitting (high model variance, low model bias) for small values of K and underfitting (low model variance, high model bias) for large values of K. In general practice, people tend to focus their grid search for K around $\sqrt{N}$.

</div>
***

***
<div id="infobox">

<center style="color:black;"> **NOTE 2** </center>

It is important to keep in mind that the distance calculation between two observations is highly dependent on the scale of measurement for the predictor variables. If predictors are on different scales, the distance metric formula will favor the differences in predictors with larger scale. This is not an ideal situation. Therefore, it is important to center and scale all predictors prior to the KNN algorithm so each predictor contributes to the distance metric calculation in the same way.

</div>
***


# 3. Prediction with K-Nearest Neighbors

Given that we learn about distance calculation and how to identify K-nearest neighbors based on a distance metric, the prediction in KNN is very simple and straightforward.

Below is a list of steps for predicting an outcome for a given observation.

1. Calculate the distance between the observation and remaining $N-1$ observations in the data (with a user choice of $q$ in Minkowski distance).

2. Rank order the observations based on the calculated distance, and choose the K-nearest neighbor. (with a user choice of $K$)

3. Calculate the mean of observed outcome in the smaller set of K-neareast neighbors as your prediction.

Note that, Step 3 applies regardless of the type of outcome. If the outcome variable is continous, we take the average outcome for the K-nearest neighbors as our prediction. If the outcome variable is binary variable (e.g., 0 vs. 1), then the proportion of observing each class among the K-nearest neighbors yield predicted probabilities for each class.

Below, I provide an example for both types of outcome using the Readability and Recidivism datasets.

## 3.1. Predicting a continuous outcome with the KNN algorithm

The code below is identical to the code we used in earlier classes for data preparation of the Readability datasets. Note that this is only to demonstrate the logic of model predictions in the context of K-nearest neighbors. So, we are using the whole dataset. In the next section, we will demonstrate the full workflow of model training and tuning with 10-fold cross-validation using the `caret::train()` function. 

1. Import the data
2. Remove variables with more than 80% missingness
3. Write a recipe for processing variables
4. Apply the recipe to the dataset

```{r, echo=TRUE,eval=FALSE,class.source='klippy',class.source = 'fold-show',message=FALSE, warning=FALSE}

# Import the dataset

readability <- read.csv('https://raw.githubusercontent.com/uo-datasci-specialization/c4-ml-fall-2021/main/data/readability_features.csv',header=TRUE)

# Remove the variables with more than 80% missingness

require(finalfit)

missing_    <- ff_glimpse(readability)$Continuous
flag_na     <- which(as.numeric(missing_$missing_percent) > 80)
readability <- readability[,-flag_na]

# Write the recipe

require(recipes)

blueprint_readability <- recipe(x     = readability,
                                vars  = colnames(readability),
                                roles = c(rep('predictor',990),'outcome')) %>%
  step_zv(all_numeric()) %>%
  step_nzv(all_numeric()) %>%
  step_impute_mean(all_numeric()) %>%
  step_normalize(all_numeric_predictors()) %>%
  step_corr(all_numeric(),threshold=0.9)

# Apply the recipe

baked_read <- blueprint_readability %>% 
  prep(training = readability) %>%
  bake(new_data = readability)

```

Our final dataset (`baked_read`) has 2834 observations and 888 columns (887 predictors, and the last column is target outcome). Now, suppose that we would like to predict the readability score for the first observation. The code below will calculate the Minkowski distance (with $q=2$) between the first observation and each of the remaining 2833 observations by using the first 887 columns of the dataset (predictors).

```{r, echo=TRUE,eval=FALSE,class.source='klippy',class.source = 'fold-show',message=FALSE, warning=FALSE}

dist <- data.frame(obs = 2:2834,dist = NA,target=NA)

for(i in 1:2833){
  
  a <- as.matrix(baked_read[1,1:887])
  b <- as.matrix(baked_read[dist[i+1,1],1:887])
  dist[i,]$dist   <- sqrt(sum((a-b)^2))
  dist[i,]$target <- baked_read[dist[i,1],]$target

  #print(i)
}

```

```{r, echo=FALSE,eval=TRUE,class.source='klippy',class.source = 'fold-show',message=FALSE, warning=FALSE}

load("B:/UO Teaching/EDLD 654/Books&Resources/largedata/knn_readability_prediction_demo.RData")
```

We now rank order the observations from closest to the most distant, and then choose the 20-nearest observations (K=20). Finally, we can calculate the average of the observed outcome for the 20-nearest neighbors, this will become our prediction of the readability score for the first observation.

```{r, echo=TRUE,eval=TRUE,class.source='klippy',class.source = 'fold-show',message=FALSE, warning=FALSE}

# Rank order the observations from closest to the most distant

dist <- dist[order(dist$dist),]

# Check the 20-nearest neighbors

dist[1:20,]
```

```{r, echo=TRUE,eval=TRUE,class.source='klippy',class.source = 'fold-show',message=FALSE, warning=FALSE}

# Mean target for the 20-nearest observations

mean(dist[1:20,]$target)
```

```{r, echo=TRUE,eval=TRUE,class.source='klippy',class.source = 'fold-show',message=FALSE, warning=FALSE}

# Check the actual observed value of reability for the first observation

readability[1,]$target

```

## 3.2. Predicting a binary outcome with the KNN algorithm

We can follow the same procedures to predict the recidivism in the second year after the initial release from prison for an individual. 

```{r, echo=TRUE,eval=TRUE,class.source='klippy',class.source = 'fold-show',message=FALSE, warning=FALSE}

# Import data

recidivism <- read.csv('https://raw.githubusercontent.com/uo-datasci-specialization/c4-ml-fall-2021/main/data/recidivism_y1 removed.csv',header=TRUE)

# Write the recipe
  

  # List of variable types
  
  outcome <- c('Recidivism_Arrest_Year2')
  
  categorical <- c('Residence_PUMA',
                   'Prison_Offense',
                   'Age_at_Release',
                   'Supervision_Level_First',
                   'Education_Level',
                   'Prison_Years',
                   'Gender',
                   'Race',
                   'Gang_Affiliated',
                   'Prior_Arrest_Episodes_DVCharges',
                   'Prior_Arrest_Episodes_GunCharges',
                   'Prior_Conviction_Episodes_Viol',
                   'Prior_Conviction_Episodes_PPViolationCharges',
                   'Prior_Conviction_Episodes_DomesticViolenceCharges',
                   'Prior_Conviction_Episodes_GunCharges',
                   'Prior_Revocations_Parole',
                   'Prior_Revocations_Probation',
                   'Condition_MH_SA',
                   'Condition_Cog_Ed',
                   'Condition_Other',
                   'Violations_ElectronicMonitoring',
                   'Violations_Instruction',
                   'Violations_FailToReport',
                   'Violations_MoveWithoutPermission',
                   'Employment_Exempt') 

  numeric   <- c('Supervision_Risk_Score_First',
                 'Dependents',
                 'Prior_Arrest_Episodes_Felony',
                 'Prior_Arrest_Episodes_Misd',
                 'Prior_Arrest_Episodes_Violent',
                 'Prior_Arrest_Episodes_Property',
                 'Prior_Arrest_Episodes_Drug',
                 'Prior_Arrest_Episodes_PPViolationCharges',
                 'Prior_Conviction_Episodes_Felony',
                 'Prior_Conviction_Episodes_Misd',
                 'Prior_Conviction_Episodes_Prop',
                 'Prior_Conviction_Episodes_Drug',
                 'Delinquency_Reports',
                 'Program_Attendances',
                 'Program_UnexcusedAbsences',
                 'Residence_Changes',
                 'Avg_Days_per_DrugTest',
                 'Jobs_Per_Year')
  
  props      <- c('DrugTests_THC_Positive',
                  'DrugTests_Cocaine_Positive',
                  'DrugTests_Meth_Positive',
                  'DrugTests_Other_Positive',
                  'Percent_Days_Employed')
  
  # Convert all nominal, ordinal, and binary variables to factors
  # Leave the rest as is
  
  for(i in categorical){
    
    recidivism[,i] <- as.factor(recidivism[,i])
    
  }
  
  # For variables that represent proportions, add/substract a small number
  # to 0s/1s for logit transformation
  
  for(i in props){
    recidivism[,i] <- ifelse(recidivism[,i]==0,.0001,recidivism[,i])
    recidivism[,i] <- ifelse(recidivism[,i]==1,.9999,recidivism[,i])
  }

  # Blueprint for processing variables
    
  require(recipes)
  
  blueprint_recidivism <- recipe(x  = recidivism,
                                 vars  = c(categorical,numeric,props,outcome),
                                 roles = c(rep('predictor',48),'outcome')) %>%
    step_indicate_na(all_of(categorical),all_of(numeric),all_of(props)) %>%
    step_zv(all_numeric()) %>%
    step_impute_mean(all_of(numeric),all_of(props)) %>%
    step_impute_mode(all_of(categorical)) %>%
    step_logit(all_of(props)) %>%
    step_ns(all_of(numeric),all_of(props),deg_free=3) %>%
    step_normalize(paste0(numeric,'_ns_1'),
                   paste0(numeric,'_ns_2'),
                   paste0(numeric,'_ns_3'),
                   paste0(props,'_ns_1'),
                   paste0(props,'_ns_2'),
                   paste0(props,'_ns_3')) %>%
    step_dummy(all_of(categorical),one_hot=TRUE)  %>%
    step_num2factor(Recidivism_Arrest_Year2,
                  transform = function(x) x + 1,
                  levels=c('No','Yes'))

# Apply the recipe

baked_recidivism <- blueprint_recidivism %>% 
  prep(training = recidivism) %>% 
  bake(new_data = recidivism)


```

The final dataset (`baked_recidivism`) has 18111 observations and 166 columns (165 predictors, and the first column is the outcome variable). Now, suppose that we would like to predict the probability of Recidivism for the first individual. The code below will calculate the Minkowski distance (with $q=2$) between the first individual and each of the remaining 18,110 individuals by using values of the 166 predictors in this dataset.

```{r, echo=TRUE,eval=FALSE,class.source='klippy',class.source = 'fold-show',message=FALSE, warning=FALSE}

dist2 <- data.frame(obs = 2:18111,dist = NA,target=NA)

for(i in 1:18110){
  
  a <- as.matrix(baked_recidivism[1,2:165])
  b <- as.matrix(baked_recidivism[dist2[i+1,1],2:165])
  dist2[i,]$dist   <- sqrt(sum((a-b)^2))
  dist2[i,]$target <- as.character(baked_recidivism[dist2[i,1],]$Recidivism_Arrest_Year2)

  #print(i)
}

```


```{r, echo=FALSE,eval=TRUE,class.source='klippy',class.source = 'fold-show',message=FALSE, warning=FALSE}

load("B:/UO Teaching/EDLD 654/Books&Resources/largedata/knn_recidivism_prediction_demo.RData")
```

We now rank order the individuals from closest to the most distant, and then choose the 100-nearest observations (K=100). Then, we calculate proportion of individuals who were recidivated (YES) and not recidivated (NO) among these 100-nearest neighbors. These proportions are the predictions for the probability of being recidivated or not recidivated for the first individual.

```{r, echo=TRUE,eval=TRUE,class.source='klippy',class.source = 'fold-show',message=FALSE, warning=FALSE}

# Rank order the observations from closest to the most distant

dist2 <- dist2[order(dist2$dist),]

# Check the 100-nearest neighbors

dist2[1:100,]
```

```{r, echo=TRUE,eval=TRUE,class.source='klippy',class.source = 'fold-show',message=FALSE, warning=FALSE}

# Mean target for the 100-nearest observations

table(dist2[1:100,]$target)

  # This indicates that the predicted probability of being recidivated is 0.30
  # for the first individual given the observed data for 100 most similar 
  # observations
```

```{r, echo=TRUE,eval=TRUE,class.source='klippy',class.source = 'fold-show',message=FALSE, warning=FALSE}

# Check the actual observed outcome for the first individual

recidivism[1,]$Recidivism_Arrest_Year2

```

# 4. Kernels to Weight the Neighbors

In the previous section, we tried to understand how KNN predicts a target outcome by simply averaing the observed value for the target outcome from K-nearest neighbors. This was a simple average by equally weighting each neighbor. 

Another way of averaging the target outcome from K-nearest neighbors would be to weight each neighbor according to its distance, and calculate a weighted average. A simple way to weight each neighbor is to use the inverse of distance. For instance, consider the earlier example where we find the 20-nearest neighbor for the first observation in the reeadability dataset.

```{r, echo=TRUE,eval=TRUE,class.source='klippy',class.source = 'fold-show',message=FALSE, warning=FALSE}

dist <- dist[order(dist$dist),]

k_neighbors <- dist[1:20,]

k_neighbors
```

We can assign a weight to each neighbor by taking the inverse of their distance and rescaling them such that the sum of the weights are equal to 1.

```{r, echo=TRUE,eval=TRUE,class.source='klippy',class.source = 'fold-show',message=FALSE, warning=FALSE}

k_neighbors$weight <- 1/k_neighbors$dist
k_neighbors$weight <- k_neighbors$weight/sum(k_neighbors$weight)


k_neighbors
```

Then, we can compute a weighted average of the target scores instead of a simple average.


```{r, echo=TRUE,eval=TRUE,class.source='klippy',class.source = 'fold-show',message=FALSE, warning=FALSE}

# Weighted Mean target for the 20-nearest observations

sum(k_neighbors$target*k_neighbors$weight)

```


There are a number of different kernel function to assign weight to K-nearest neighbors (e.g., epanechnikov, quartic, triweight, tricube, gaussian, cosine). For all of them, closest neighbors are assigned higher weights while the weight gets smaller as the distance increases. They slightly differ the way they assign the weight. Below is a demonstration how assigned weight changes as a function of distance for different kernel functions.

```{r, echo=FALSE,eval=TRUE,class.source='klippy',class.source = 'fold-show',message=FALSE, warning=FALSE, fig.width=8,fig.height=5}

require(rdd)

X   <-seq(0.01,1,.01)

wts.epanechnikov <-kernelwts(X,0,1,kernel="epanechnikov")
wts.quartic      <-kernelwts(X,0,1,kernel="quartic")
wts.triweight    <-kernelwts(X,0,1,kernel="triweight")
wts.tricube      <-kernelwts(X,0,1,kernel="tricube")
wts.gaussian     <-kernelwts(X,0,1,kernel="gaussian")
wts.cosine       <-kernelwts(X,0,1,kernel="cosine")
wts.tri       <-kernelwts(X,0,1,kernel="triangular")



p1 <- ggplot() + geom_line(aes(x=X,y=wts.epanechnikov)) + theme_bw() + xlab('Distance')+ylab('Weight')+ggtitle('Epanechnikov')+ylim(c(0,.025))

p2 <- ggplot() + geom_line(aes(x=X,y=wts.quartic)) + theme_bw() + xlab('Distance')+ylab('Weight')+ggtitle('Quartic')+ylim(c(0,.025))

p3 <- ggplot() + geom_line(aes(x=X,y=wts.triweight)) + theme_bw() + xlab('Distance')+ylab('Weight')+ggtitle('Triweight')+ylim(c(0,.025))

p4 <- ggplot() + geom_line(aes(x=X,y=wts.tricube)) + theme_bw() + xlab('Distance')+ylab('Weight')+ggtitle('Tricube')+ylim(c(0,.025))

p5 <- ggplot() + geom_line(aes(x=X,y=wts.gaussian)) + theme_bw() + xlab('Distance')+ylab('Weight')+ggtitle('Gaussian')+ylim(c(0,.025))

p6 <- ggplot() + geom_line(aes(x=X,y=wts.cosine)) + theme_bw() + xlab('Distance')+ylab('Weight')+ggtitle('Cosine')+ylim(c(0,.025))

p7 <- ggplot() + geom_line(aes(x=X,y=wts.tri)) + theme_bw() + xlab('Distance')+ylab('Weight')+ggtitle('Triangular')+ylim(c(0,.025))

grid.arrange(p1,p2,p3,p4,p5,p6,p7,nrow=3)

```



***
<div id="infobox">

<center style="color:black;"> **NOTE 3** </center>

Which kernel function should we use for weighting the distance? The type of kernel function can also be considered as a hyperparameter to tune.

</div>
***

# 5. Parallel Processing with the `caret::train()` function

The KNN algorithm can become computationally quite intensive and grid search with so many combinations may take days particularly when you have a very large sample size. One way to reduce the computational time and make it reasonable is to use parallel processing when you have access to multiple core computers (or computing clusters).

The `caret` package is designed in a way to take advantage of multiple cores in a computer via another package `doParallel`. If you are worried about the computation time for fitting any model using the `caret::train` function, you can run the following lines of code to use as many cores as available in your computing device.

```{r, echo=TRUE,eval=FALSE,class.source='klippy',class.source = 'fold-show',message=FALSE, warning=FALSE}

require(doParallel)

ncores <- 15    # depends on the number of cores available in your computer

cl <- makePSOCKcluster(ncores)

registerDoParallel(cl)

```

Once you run these lines of code, anytime you run the `caret::train` function to train any model, it will use all registered cores to do the computations. This could significantly reduce the computational time from days to hours or from hours to minutes.

After you are done with training the model, you can run the following line of code to stop the computational cluster registered for the use of your current R session.

```{r, echo=TRUE,eval=FALSE,class.source='klippy',class.source = 'fold-show',message=FALSE, warning=FALSE}
stopCluster(cl)
```

You can find more information about parallel processing with the caret package at [this link](https://topepo.github.io/caret/parallel-processing.html).

# 6. K-Nearest Neighbors Algorithm to Predict Readability Scores 

In this section, we implement the KNN algorithm to build a prediction model for the readability score. The first part of the code is identical to the code we use in earlier classes for the following steps:

1. Split the data into train and test sets
2. Create the list of row indices for 10-fold cross validation
3. Set the object for cross validation settings.


```{r, echo=TRUE,eval=FALSE,class.source='klippy',class.source = 'fold-show',message=FALSE, warning=FALSE}


# Train and Test Split

  set.seed(10152021)  # for reproducibility
    
  loc      <- sample(1:nrow(readability), round(nrow(readability) * 0.9))
  read_tr  <- readability[loc, ]
  read_te  <- readability[-loc, ]
  
# Create the row indices for 10-folds
  
    # Randomly shuffle the training data

      read_tr = read_tr[sample(nrow(read_tr)),]

    # Create 10 folds with equal size

      folds = cut(seq(1,nrow(read_tr)),breaks=10,labels=FALSE)
  
    # Create the list for each fold 
      
      my.indices <- vector('list',10)
      for(i in 1:10){
        my.indices[[i]] <- which(folds!=i)
      }

# Cross-validation settings
      
  cv <- trainControl(method = "cv",
                     index  = my.indices)
```


We will use the `kknn` package to implement the KNN algorithm, and this method is available through the `caret::train()` function. First, let's check which hyperparameters are available to tune for this method.

```{r, echo=TRUE,eval=TRUE,class.source='klippy',class.source = 'fold-show',message=FALSE, warning=FALSE}

# install.package('kknn')

require(caret)
require(kknn)


getModelInfo()$kknn$parameters
```

There are three hyperparameters available to optimize for the `kknn` method. These are the same parameters discussed earlier:

- `kmax`: the choice of K for the K-nearest neighbors (numeric). 
- `distance`: the choice of power value ($q$) for the Minkowski distance (numeric)
- `kernel`: the choice of kernel function for weigted predictions (character)

The possible kernel functions available are "rectangular","triangular", "epanechnikov", "biweight", "triweight", "cos", "inv", and "gaussian". The "rectangular" indicates a simple average with no weight applied (equally weighted).

The next step is to create a matrix for hyperparameter grid search. Let's consider the following values for these three hyperparameters:

- `kmax`: 2, 3, 4,..., 25 
- `distance`: 1, 2, 3
- `kernel`: rectangular, epanechnikov

```{r, echo=FALSE,eval=TRUE,class.source='klippy',class.source = 'fold-show',message=FALSE, warning=FALSE}

load("B:/UO Teaching/EDLD 654/Books&Resources/largedata/knn_readability_caretfit.RData")

```

```{r, echo=TRUE,eval=TRUE,class.source='klippy',class.source = 'fold-show',message=FALSE, warning=FALSE}

# Hyperparameter Tuning Grid
  
  grid <- expand.grid(kmax    = 3:25,
                     distance = c(1,2,3),
                     kernel   = c('epanechnikov','rectangular'))
  grid
```

There are a total of 138 combinations we would like to search. Note that, this means a total of 1380 model fitted with 10-fold cross validation for each combination. Also, the KNN algorithm is very slow to fit (because you have to calculate N*(N-1)/2 distances). This may take very long. Therefore, I will also use parallel processing with 15 cores.

```{r, echo=TRUE,eval=FALSE,class.source='klippy',class.source = 'fold-show',message=FALSE, warning=FALSE}

require(doParallel)

ncores <- 15    # depends on the number of cores available in your computer

cl <- makePSOCKcluster(ncores)

registerDoParallel(cl)

```

Now, we can fit the model with these settings.

```{r, echo=TRUE,eval=FALSE,class.source='klippy',class.source = 'fold-show',message=FALSE, warning=FALSE}

# Train the model

  caret_knn_readability <- caret::train(blueprint_readability, 
                                        data      = read_tr,
                                        method    = "kknn",
                                        trControl = cv,
                                        tuneGrid  = grid)

  caret_knn_readability$times
```

```{r, echo=FALSE,eval=TRUE,class.source='klippy',class.source = 'fold-show',message=FALSE, warning=FALSE}

caret_knn_readability$times
```

The training took 11534.15 seconds (~ 3 hours 12 minutes) with 15 cores running at the same time (!).


Let's check the best combination of hyperparameters that optimizes the 10-fold cross validated performance metric (RMSE).

```{r, echo=TRUE,eval=TRUE,class.source='klippy',class.source = 'fold-show',message=FALSE, warning=FALSE}

plot(caret_knn_readability)

caret_knn_readability$bestTune
```

Now, we can check the performance of the KNN algorithm on the test dataset.

```{r, echo=TRUE,eval=TRUE,class.source='klippy',class.source = 'fold-show',message=FALSE, warning=FALSE}

predicted_te <- predict(caret_knn_readability,read_te)

# R-square

cor(read_te$target,predicted_te)^2

# RMSE

sqrt(mean((read_te$target - predicted_te)^2))

# MAE

mean(abs(read_te$target - predicted_te))


```

The table below provides a comparison to the models we discussed earlier.


|                   | R-square | MAE   | RMSE
|-------------------|:--------:|:-----:|:-----:|
| Linear Regression |  0.644   | 0.522 | 0.644 |
| Ridge Regression  |  0.727   | 0.435 | 0.536 |
| Lasso Regression  |  0.725   | 0.434 | 0.538 |
| KNN               |  0.623   | 0.500 | 0.629 |  


```{r, echo=FALSE,eval=FALSE,class.source='klippy',class.source = 'fold-show',message=FALSE, warning=FALSE}

# Train and Test Split 

  loc <- which(recidivism$Training_Sample==1)
  
  recidivism_tr  <- recidivism[loc, ]
  recidivism_te  <- recidivism[-loc, ]


# Randomly shuffle the data

  set.seed(10302021) # for reproducibility
  
  recidivism_tr = recidivism_tr[sample(nrow(recidivism_tr)),]

# Create 10 folds with equal size

  folds = cut(seq(1,nrow(recidivism_tr)),breaks=10,labels=FALSE)

# Create the list for each fold 

  my.indices <- vector('list',10)
  for(i in 1:10){
    my.indices[[i]] <- which(folds!=i)
  }

# Caret package
  
  require(caret)

  # Check the kknn engine hyperparameters
  
    # getModelInfo()$kknn$parameters

  # Cross-validation settings
  
  cv <- trainControl(method   = "cv",
                     index           = my.indices,
                     classProbs      = TRUE,
                     summaryFunction = mnLogLoss)

  # Hyperparameter Tuning Grid
  
  grid <- data.frame(kmax=3:150,
                     distance=2,
                     kernel='rectangular') 
  grid


# Train the model

  caret_knn <- caret::train(blueprint_recidivism, 
                            data      = recidivism_tr,
                            method    = "kknn",
                            metric    = 'logLoss',
                            trControl = cv,
                            tuneGrid  = grid)

```

## Variable Importance

You can ask for the variable importance plot from the KNN algorithmsimilar to other models (although I am not sure how they calculate the variable importance in the context of KNN).

```{r, echo=TRUE,eval=TRUE,class.source='klippy',class.source = 'fold-show',message=FALSE, warning=FALSE,fig.width=8,fig.height=6}

require(vip)

vip(caret_knn_readability, 
    num_features = 10, 
    geom = "point") + 
  theme_bw()

```
