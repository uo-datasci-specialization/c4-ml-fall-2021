---
title: Regularization in Linear Regression
subtitle: Applied Machine Learning for Educational Data Science
author:
  name: Cengiz Zopluoglu
  affiliation: University of Oregon | EDLD 654
date: 10/20/2021 ## Or "Lecture no."
output: 
  html_document:
    keep_md: false
    theme: journal
    highlight: haddock
    code_folding: hide
    toc: yes
    toc_depth: 4
    toc_float: yes
  pdf_document:
    extra_dependencies: ["amssymb","animate","upgreek","amsmath"]
    keep_tex: false ## Change to true if want keep intermediate .tex file
    toc: true
    toc_depth: 3
    dev: cairo_pdf
always_allow_html: true
urlcolor: blue
mainfont: cochineal
sansfont: Fira Sans
monofont: Fira Code ## Although, see: https://tex.stackexchange.com/q/294362

## Automatically knit to both formats:
knit: (function(inputFile, encoding) {
 rmarkdown::render(inputFile, encoding = encoding, 
 output_format = 'all') 
 })
---

```{r klippy, echo=FALSE, include=TRUE}
klippy::klippy(position=c('top','right'))
```

`r paste('[Updated:',format(Sys.time(),'%a, %b %d, %Y - %H:%M:%S'),']')`

<style>
  
  .list-group-item.active, .list-group-item.active:focus, .list-group-item.active:hover {
    z-index: 2;
    color: #fff;
      background-color: #FC4445;
      border-color: #97CAEF;
  }

#infobox {
padding: 1em 1em 1em 4em;
margin-bottom: 10px;
border: 2px solid black;
border-radius: 10px;
background: #E6F6DC 5px center/3em no-repeat;
  }

</style>
  
  
```{r setup, include=FALSE}
knitr::opts_chunk$set(comment = "",fig.align='center')
require(here)
require(ggplot2)
require(plot3D)
require(kableExtra)
require(knitr)
require(giski)
require(magick)
require(gridExtra)
require(plotly)
options(scipen=99)


# Resources:

# https://bradleyboehmke.github.io/HOML/linear-regression.html
# https://bradleyboehmke.github.io/HOML/regularized-regression.html
# James et al. Ch 3 and Ch 6.2
# Applied Predictive Modeling, Chapter 6
```

# Regularization

Regularization is a general strategy to incorporate additional penalty terms into the model fitting process and used not just for regression but a variety of other types of models. The idea behind the regularization is to constrain the size of regression coefficients with the purpose of reducing their sampling variation and, hence, reducing the variance of model predictions. These constrains are typically incorporated into the loss function to be optimized. There are two commonly used regularization strategy: **ridge penalty** and **lasso penalty**. In addition, there is also **elastic net**, a mixture of these two strategies.

## Ridge Regression

### Ridge Penalty

Remember that we formulated the loss function for the linear regression as the sum of squared residuals across all observations. For ridge regression, we add a penalty term to this loss function and this penalty term is a function of all the regression coefficients in the model. Assuming that there are P regression coefficients in the model, the penalty term for the ridge regression would be 

$$\lambda \sum_{i=1}^{P}\beta_p^2,$$
where $\lambda$ is a parameter that penalizes the regression coefficients when they get larger. Therefore, when we fit a regression model with ridge penalty, the loss function to minimize becomes

$$Loss = \sum_{i=1}^{N}\epsilon_{(i)}^2 + \lambda \sum_{i=1}^{P}\beta_p^2,$$
$$Loss = SSR + \lambda \sum_{i=1}^{P}\beta_p^2.$$

Let's consider the same example from the previous class. Suppose we fit a simple linear regression model such that the readability score is the outcome ($Y$) and average word length is the predictor($X$). Our regression model is

$$Y = \beta_0  + \beta_1X + \epsilon,$$
and let's assume the set of coefficients are {$\beta_0,\beta_1$} = {7.5,-2}, so my model is
$$Y = 7.5  - 2X + \epsilon.$$
Then, the value of the loss function when $\lambda=0.2$ would be equal to 27.433.

```{r, echo=TRUE,eval=TRUE,class.source='klippy',class.source = 'fold-show',message=FALSE, warning=FALSE,fig.width=8,fig.height=8}

readability_sub <- read.csv('https://raw.githubusercontent.com/uo-datasci-specialization/c4-ml-fall-2021/main/data/readability_feat_sub.csv',header=TRUE)

d <-  readability_sub[,c('mean.wl','target')]

b0 = 7.5
b1 = -2

d$predicted <- b0 + b1*d$mean.wl
d$error     <- d$target - d$predicted

d

lambda = 0.2

loss <- sum((d$error)^2) + lambda*(b0^2 + b1^2)

loss 
```

Notice that when $\lambda$ is equal to 0, the loss function is identical to SSR; therefore, it becomes a linear regression with no regularization. As the value of $\lambda$ increases, the degree of penalty linearly increases. Technically, the  $\lambda$ can take any positive value between 0 and $\infty$.

As we did in the previous lecture, imagine that we computed the loss function with the ridge penalty term for every possible combination of the intercept ($\beta_0$) and the slope ($\beta_1$). Let's say the plausible range for the intercept is from -10 to 10 and the plausible range for the slope is from -2 to 2. Now, we also have to think different values of $\lambda$ because the surface we try to minimize is dependent on the value $\lambda$ and different values of $\lambda$ yield different estimates of $\beta_0$ and and $\beta_1$. 

You can try a number of different values for $\lambda$ using the shiny app at [this link](https://cengiz-shiny.app/shiny/ridge/) and explore how the loss function value and coefficient estimates change for different values of $\lambda$. Note that when $\lambda$ is equal to zero, it should be equivalent of what we have seen in the earlier lecture. Try values of 1, 5, 10, 50, and 100.

```{r, echo=FALSE,eval=FALSE,class.source='klippy',class.source = 'fold-show',message=FALSE, warning=FALSE,fig.width=8,fig.height=8}

setwd('B:/UO Teaching/EDLD 654/c4-ml-fall-2021/website-data/animations/ridge')

require(plotly)

lambda = seq(0,100,.05)

b0 <- c()
b1 <- c()

for(i in 1:length(lambda)){
  
     grid    <- expand.grid(b0=seq(-10,10,.1),b1=seq(-5,5,.01))           
     grid$SSR <- NA
      
      B1    <- matrix(grid$b1,ncol=20,nrow=nrow(grid),byrow=FALSE)
      B0    <- matrix(grid$b0,ncol=20,nrow=nrow(grid),byrow=FALSE)
      X     <- matrix(d$mean.wl,ncol=20,nrow=nrow(grid),byrow=TRUE)
      Y_hat <- B0 + X*B1
      Y     <- matrix(d$target,ncol=20,nrow=nrow(grid),byrow=TRUE)
      P     <- lambda[i]*(grid$b0^2 + grid$b1^2)
      grid$SSR <- rowSums((Y - Y_hat)^2) + P

      fig <- plot_ly(grid, x = ~b0, y = ~b1, z = ~SSR, 
              marker = list(color = ~SSR,
                            showscale = FALSE,
                            cmin=min(grid$SSR),
                            cmax=max(grid$SSR),cauto=F),
              width=1200,height=1200) %>% 
        add_markers() %>%
        layout(title = paste0("lambda = ",lambda[i]))%>%
        layout(scene = list(xaxis=list(title = 'Beta0'),
                            yaxis=list(title = 'Beta1'),
                            camera = list(eye = list(x = 2, y = 0.5, z = 0.5),
                                          up  = list(x=0,y=0.5,z=0.5)))) %>% 
        config(mathjax = 'cdn')
      

      orca(fig,paste0('plot',i,'.png'))
      
      
      b0[i] <- grid[which.min(grid$SSR),]$b0
      b1[i] <- grid[which.min(grid$SSR),]$b1
      
      
      p1 <- ggplot() +
        geom_point(aes(x=lambda[1:i],y=b0),cex=0.5) +
        geom_line(aes(x=lambda[1:i],y=b0),size=0.25) +
        xlim(c(0,100))+
        ylim(c(0,5)) +
        xlab('lambda')+
        ylab('Beta0')+
        theme_bw()
        
      p2 <- ggplot() +
        geom_point(aes(x=lambda[1:i],y=b1),cex=0.5) +
        geom_line(aes(x=lambda[1:i],y=b1),size=0.25) +
        xlim(c(0,100))+
        ylim(c(-1.5,0)) +
        xlab('lambda')+
        ylab('Beta1')+
        theme_bw()
      
      ggsave(paste0('b0plot',i,'.png'),p1,width = 1200,height=600,unit='px')
      ggsave(paste0('b1plot',i,'.png'),p2,width = 1200,height=600,unit='px')
      
}

require(magick)

for(i in 1:2001){

  a = image_read(paste0('b0plot',i,'.png'))
  b = image_read(paste0('b1plot',i,'.png'))
  p = image_read(paste0('plot',i,'.png'))

  all <- image_append(c(p,image_append(c(a,b),stack=TRUE)))

  image_write(all,
              path = paste0('image',i,'.png'),
              format='png',
              quality=100,
              depth=16)
  print(i)
}

imgs <- list.files(full.names=TRUE)
loc <- grep('./image',imgs)
imgs <- imgs[loc]

imgs <- imgs[order(as.numeric(substring(imgs,
                                        str_locate(imgs,'./image')[,2]+1,
                                        str_locate(imgs,'.png')[,1]-1)))]

imgs <- imgs[c(1:50,seq(51,2001,100))]

img_list <- lapply(imgs, image_read)
img_joined <- image_join(img_list)
img_animated <- image_animate(img_joined, fps = 10)
#img_animated
image_write(image = img_animated,
            path = 'ridge.gif')
```

Below is also a demonstration of what happens to loss function and the regression coefficients for increasing levels of $\lambda$.

```{r, echo=FALSE,eval=knitr::is_html_output(),class.source='klippy',fig.align='center',fig.height=8,fig.width=8}

knitr::include_graphics(here('website-data/animations/ridge/ridge.gif'))

```

### Model Estimation

#### Matrix Solution

The matrix solution we learned before for regression without regularization can also be applied to estimate the coeffients from ridge regression given the $\lambda$ value. Given that 

- $\mathbf{Y}$ is an N x 1 column vector of observed values for the outcome variable, 
- $\mathbf{X}$ is an N x (P+1) **design matrix* for the set of predictor variables including an intercept term,
- $\boldsymbol{\beta}$ is an (P+1) x 1 column vector of regression coefficients, 
- $\mathbf{I}$ is a (P+1) x (P+1) identity matrix,
- and $\lambda$ is positive real-valued number,

the set of ridge regression coefficients can be estimated using the following matrix operation.

$$\hat{\boldsymbol{\beta}} = (\mathbf{X^T}\mathbf{X} + \lambda \mathbf{I})^{-1}\mathbf{X^T}\mathbf{Y}$$
Now, suppose we want to predict the readability score by using the two predictors, the average word length ($X_1$) and number of sentences ($X_2$). Our model will be

$$Y_{(i)} = \beta_0  + \beta_1X_{1(i)} + \beta_2X_{2(i)} + \epsilon_{(i)}.$$
If we estimate the ridge regression coefficients by using $\lambda=.5$, the estimates would be {$\beta_0,\beta_1,\beta_2$} = {0.277,-.593,0.097}.

```{r, echo=TRUE,eval=TRUE,class.source='klippy',class.source = 'fold-show',message=FALSE, warning=FALSE,fig.width=8,fig.height=8}

Y <-  as.matrix(readability_sub$target)
X <-  as.matrix(cbind(1,readability_sub$mean.wl,readability_sub$sents))

lambda <- 0.5

beta <- solve(t(X)%*%X + lambda*diag(ncol(X)))%*%t(X)%*%Y

beta 
```

If we change the value of $\lambda$ to 2, then we will get a different set of estimates for the regression coefficients.

```{r, echo=TRUE,eval=TRUE,class.source='klippy',class.source = 'fold-show',message=FALSE, warning=FALSE,fig.width=8,fig.height=8}

Y <-  as.matrix(readability_sub$target)
X <-  as.matrix(cbind(1,readability_sub$mean.wl,readability_sub$sents))

lambda <- 2

beta <- solve(t(X)%*%X + lambda*diag(ncol(X)))%*%t(X)%*%Y

beta 
```

We can manipulate the value of $\lambda$ from 0 to 100 with increments of .1 and calculate the regression coefficients for every possible value of $\lambda$. Note the regression coefficients will shrink towards zero, but will never be exactly equal to zero in ridge regression.

```{r, echo=TRUE,eval=TRUE,class.source='klippy',class.source = 'fold-show',message=FALSE, warning=FALSE,fig.width=8,fig.height=5}

Y <-  as.matrix(readability_sub$target)
X <-  as.matrix(cbind(1,readability_sub$mean.wl,readability_sub$sents))

lambda <- seq(0,100,.01)

beta     <- data.frame(matrix(nrow=length(lambda),ncol=4))
beta[,1] <- lambda

for(i in 1:length(lambda)){
  beta[i,2:4] <- t(solve(t(X)%*%X + lambda[i]*diag(ncol(X)))%*%t(X)%*%Y)
}

ggplot(data = beta)+
  geom_line(aes(x=X1,y=X2))+
  geom_line(aes(x=X1,y=X3))+
  geom_line(aes(x=X1,y=X4))+
  xlab(expression(lambda))+
  ylab('')+
  theme_bw()+
  annotate(geom='text',x=1.5,y=1.5,label=expression(beta[0]))+
  annotate(geom='text',x=2,y=.15,label=expression(beta[2]))+
  annotate(geom='text',x=1.5,y=-.9,label=expression(beta[1]))
  
```

#### Ridge Regression with Standardized Variables

We didn't consider a very important issue while we discussed the model estimation. This issue is not necessarily important if you only one predictor. However, it is critial whenever you have more than one predictor. Different variables have different scales and therefore the magnitude of the regression coefficients for different variables will be dependent on the scales of the variables. A regression coefficient for a variable with a range from 0 to 100 will be very different than a regression coefficient for a variable with a range from 0 to 1.

#### `glmnet()` function

### Tuning the Hyperparameter $\lambda$ without Cross-validation

The $\lambda$ parameter in ridge regression is called a **hyperparameter**. In the context of machine learning, the parameters in a model can be classified into two types: parameters and hyperparameters. The parameters of the model are typically estimated from data and not set by us. In the context of regularized regression, regression coefficients, {$\beta_0,\beta_1,...,\beta_P$},

### Tuning the Hyperparameter $\lambda$ with 10-fold Cross-validation

### Using Ridge Regression to Predict Readability Scores

## Lasso Regression

### Lasso Penalty

## Elastic Net












