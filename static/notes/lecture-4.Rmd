---
title: Linear Regression and Regularization
subtitle: Applied Machine Learning for Educational Data Science
author:
  name: Cengiz Zopluoglu
  affiliation: University of Oregon | EDLD 654
date: 10/12/2021 ## Or "Lecture no."
output: 
  html_document:
    keep_md: false
    theme: journal
    highlight: haddock
    code_folding: hide
    toc: yes
    toc_depth: 4
    toc_float: yes
  pdf_document:
    extra_dependencies: ["amssymb","animate","upgreek","amsmath"]
    keep_tex: false ## Change to true if want keep intermediate .tex file
    toc: true
    toc_depth: 3
    dev: cairo_pdf
always_allow_html: true
urlcolor: blue
mainfont: cochineal
sansfont: Fira Sans
monofont: Fira Code ## Although, see: https://tex.stackexchange.com/q/294362

## Automatically knit to both formats:
knit: (function(inputFile, encoding) {
 rmarkdown::render(inputFile, encoding = encoding, 
 output_format = 'all') 
 })
---

```{r klippy, echo=FALSE, include=TRUE}
klippy::klippy(position=c('top','right'))
```

`r paste('[Updated:',format(Sys.time(),'%a, %b %d, %Y - %H:%M:%S'),']')`

<style>

.list-group-item.active, .list-group-item.active:focus, .list-group-item.active:hover {
    z-index: 2;
    color: #fff;
    background-color: #FC4445;
    border-color: #97CAEF;
}

#infobox {
  padding: 1em 1em 1em 4em;
  margin-bottom: 10px;
  border: 2px solid black;
  border-radius: 10px;
  background: #E6F6DC 5px center/3em no-repeat;
}

</style>


```{r setup, include=FALSE}
knitr::opts_chunk$set(comment = "",fig.align='center')
require(here)
require(ggplot2)
require(plot3D)
require(kableExtra)
require(knitr)
require(giski)
require(magick)
require(gridExtra)
require(plotly)
options(scipen=99)


# Resources:

  # https://bradleyboehmke.github.io/HOML/linear-regression.html
  # https://bradleyboehmke.github.io/HOML/regularized-regression.html
  # James et al. Ch 3 and Ch 6.2
  # Applied Predictive Modeling, Chapter 6
```

In the machine learning literature, the prediction algorithms are classified into two main categories: *supervised* and *unsupervised*. Supervised algorithms are being used when the dataset has an actual outcome of interest to predict (labels), and the goal is to build the "best" model predicting the outcome of interest that exists in the data. On the other side, unsupervised algorithms are being used when the dataset doesn't have an outcome of interest, and the goal is typically to identify similar groups of observations (rows of data) or similar groups of variables (columns of data) in data. In this course, we plan to cover a number of *supervised* algorithms. Linear regression is one of the simplest approach among supervised algorithms, and also one of the easiest to interpret. 

# Linear Regression

## Model Description

In most general terms, the linear regression model with $P$ predictors ($X_1$,$X_2$,$X_3$,...,$X_p$) to predict an outcome (Y) can be written as the following:

$$ Y = \beta_0  + \sum_{p=1}^{P} \beta_pX_{p} + \epsilon.$$
In this model, $Y$ represents the observed value for the outcome for an observation, $X_{p}$ represents the observed value of the $p^{th}$ variable for the same observation, and $\beta_p$ is the associated model parameter for the $p^{th}$ variable. $\epsilon$ is the model error (residual) for the observation.

This model includes only the main effects of each predictor and can be easily extended by including a quadratic or higher-order polynomial terms for all (or a specific subset of) predictors. For instance, the model below includes all first-order, second-order, and third-order polynomial terms for all predictors. 

$$ Y = \beta_0  + \sum_{p=1}^{P} \beta_pX_{p} + \sum_{k=1}^{P} \beta_{k+P}X_{k}^2 + \sum_{m=1}^{P} \beta_{m+2P}X_{m}^3 + \epsilon.$$
The simple first-order, second-order, and third-order polynomial terms can also be replaced by corresponding terms obtained from B-splines or natural splines.

Sometimes, the effect of predictor variables on the outcome variable are not additive, and the effect of one predictor on the response variable can depend on the levels of another predictor. These non-additive effects are also called interaction effects. The interaction effects can also be a first-order interaction (interaction between two variables, e.g., $X_1*X_2$), second-order interaction ($X_1*X_2*X_3$), or higher orders.  It is also possible to add the interaction effects to the model. For instance, the model below also adds the first-order interactions.

$$ Y = \beta_0  + \sum_{p=1}^{P} \beta_pX_{p} + \sum_{k=1}^{P} \beta_{k+P}X_{k}^2 + \sum_{m=1}^{P} \beta_{m+2P}X_{m}^3 + \sum_{i=1}^{P}\sum_{j=i+1}^{P}\beta_{i,j}X_iX_j + \epsilon.$$
If you are not comfortable or confused with notational representation, below is an example for different models you can write with 5 predictors ($X_1,X_2,X_3$).

A model with only main-effects:

$$ Y = \beta_0  + \beta_1X_{1} + \beta_2X_{2} + \beta_3X_{3}+ \epsilon.$$

A model with polynomial terms up to the 3rd degree added:

$$Y = \beta_0  + \beta_1X_1 + \beta_2X_2 + \beta_3X_3 + \\ \beta_4X_1^2 + \beta_5X_2^2 + \beta_6X_3^2+ \\ \beta_{7}X_1^3 + \beta_{8}X_2^3 + \beta_{9}X_3^3$$

A model with both interaction terms and polynomial terms up to the 3rd degree added:

$$Y = \beta_0  + \beta_1X_1 + \beta_2X_2 + \beta_3X_3 + \\ \beta_4X_1^2 + \beta_5X_2^2 + \beta_6X_3^2+ \\ \beta_{7}X_1^3 + \beta_{8}X_2^3 + \beta_{9}X_3^3+ \\ \beta_{1,2}X_1X_2+ \beta_{1,3}X_1X_3 + \beta_{2,3}X_2X_3 + \epsilon$$

## Model Estimation

Suppose that we would like to predict the target readability score for a given text from average word length in the text. Below is a scatterplot to show the relationship between these two variables for a random sample of 20 observations. There seems to be a moderate negative correlation. So, we can tell that the higher the average word length is in a given text, the lower the readability score (more difficult to read).

```{r, echo=TRUE,eval=TRUE,class.source='klippy',class.source = 'fold-show',message=FALSE, warning=FALSE}

readability_sub <- read.csv('https://raw.githubusercontent.com/uo-datasci-specialization/c4-ml-fall-2021/main/data/readability_feat_sub.csv',header=TRUE)


```

```{r, echo=FALSE,eval=TRUE,class.source='klippy',class.source = 'fold-show',message=FALSE, warning=FALSE,fig.width=8,fig.height=8}

ggplot(data=readability_sub,aes(x=mean.wl,y=target))+
  geom_point()+
  xlab('Average Word Length')+
  ylab('Readability Score')+
  theme_bw()+
  xlim(c(3,6))+
  ylim(c(-4,2))

```

Let's consider a simple linear regression model such that the readability score is the outcome ($Y$) and average word length is the predictor($X1$). Our regression model would be

$$Y_i = \beta_0  + \beta_1X_{i} + \epsilon_i.$$
In this case, the set of coefficients, {$\beta_0,\beta_1$}, represents a linear line. We can come up with any set of {$\beta_0,\beta_1$} coefficients and use it as our model. For instance, suppose I guesstimate that these coefficients are {$\beta_0,\beta_1$} = {7.5,-2}. Then, my model would look like the following.

$$Y_i = 7.5  - 2X_{i} + \epsilon_i.$$

```{r, echo=FALSE,eval=TRUE,class.source='klippy',class.source = 'fold-show',message=FALSE, warning=FALSE,fig.width=8,fig.height=8}

ggplot(data=readability_sub,aes(x=mean.wl,y=target))+
  geom_point()+
  xlab('Average Word Length')+
  ylab('Readability Score')+
  theme_bw()+
  xlim(c(3,6))+
  ylim(c(-4,2))+
  geom_abline(intercept=7.5,slope=-2,lty=2)
  
```

Using this model, I can predict the target readability score for all the observation in my dataset. For instance, the average word length is 4.604 for the first reading passage. Then, my prediction of readability score based on this model would be -1.708. On the other side, the observed value of the target score for this observation is -2.586. This discrepancy between the observed value and my model predicts is the model error (residual) for the first observation and captured in the $\epsilon$ term in the model.

$$Y_1 = 7.5  - 2X_{1} + \epsilon_1.$$
$$\hat{Y_1} = 7.5  - 2*4.604 = -1.708 $$
$$\hat{\epsilon_1} = -2.586 - (-1.708) =  -0.878 $$
We can visualize this in the plot. The black dot represents the observed data point, and the blue dot on the line represents the model prediction for a given $X$ value. The vertical distance between these two data points is the model error for this particular observation.

```{r, echo=FALSE,eval=TRUE,class.source='klippy',class.source = 'fold-show',message=FALSE, warning=FALSE,fig.width=8,fig.height=8}

x1 = readability_sub[1,c('mean.wl')]
y1 = readability_sub[1,c('mean.wl')]*-2 + 7.5

ggplot(data=readability_sub,aes(x=mean.wl,y=target))+
  geom_point(col='gray')+
  xlab('Average Word Length')+
  ylab('Readability Score')+
  theme_bw()+
  xlim(c(3,6))+
  ylim(c(-4,2))+
  geom_abline(intercept=7.5,slope=-2,lty=2)+
  geom_point(x=x1,y=y1,color='blue',cex=2)+
  geom_point(x=readability_sub[1,c('mean.wl')],y=readability_sub[1,c('target')],color='black',cex=2)+
  geom_segment(x=x1,y=y1,xend=x1,yend=readability_sub[1,c('target')])
  
  
```

We can do the same experiment for the second observation. The average word length is 3.830 for the second reading passage. The model predicts a readability score of be -0.161. Observed value of the target score for this observation is 0.459. Therefore the model error for the second observation would be 0.62.


$$Y_2 = 7.5  - 2X_{2} + \epsilon_2.$$
$$\hat{Y_2} = 7.5  - 2*3.830 = -0.161 $$
$$\hat{\epsilon_2} = 0.459 - (-0.161) =  0.62 $$

```{r, echo=FALSE,eval=TRUE,class.source='klippy',class.source = 'fold-show',message=FALSE, warning=FALSE,fig.width=8,fig.height=8}

x1 = readability_sub[2,c('mean.wl')]
y1 = readability_sub[2,c('mean.wl')]*-2 + 7.5

ggplot(data=readability_sub,aes(x=mean.wl,y=target))+
  geom_point(col='gray')+
  xlab('Average Word Length')+
  ylab('Readability Score')+
  theme_bw()+
  xlim(c(3,6))+
  ylim(c(-4,2))+
  geom_abline(intercept=7.5,slope=-2,lty=2)+
  geom_point(x=x1,y=y1,color='blue',cex=2)+
  geom_point(x=readability_sub[2,c('mean.wl')],y=readability_sub[2,c('target')],color='black',cex=2)+
  geom_segment(x=x1,y=y1,xend=x1,yend=readability_sub[2,c('target')])
  
```

Using a similar approach, we can calculate the model error for every single observation.

```{r, echo=TRUE,eval=TRUE,class.source='klippy',class.source = 'fold-show',message=FALSE, warning=FALSE,fig.width=8,fig.height=8}

d <-  readability_sub[,c('mean.wl','target')]

d$predicted <- d$mean.wl*-2 + 7.5
d$error     <- d$target - d$predicted

d
 
```


```{r, echo=FALSE,eval=TRUE,class.source='klippy',class.source = 'fold-show',message=FALSE, warning=FALSE,fig.width=8,fig.height=8}

p <- ggplot(data=d,aes(x=mean.wl,y=target))+
  geom_point()+
  xlab('Average Word Length')+
  ylab('Readability Score')+
  theme_bw()+
  xlim(c(3,6))+
  ylim(c(-4,2))+
  geom_abline(intercept=7.5,slope=-2,lty=2,color='gray')
 
for(i in 1:nrow(d)){
 p <- p + geom_segment(x=d[i,1],y=d[i,2],xend=d[i,1],yend=d[i,3],lty=2)
}  

p
```

While it is helpful to see the model error for every single observation, we will need to aggregate them in some way to form an overall measure of the total amount of error for this model. Some alternatives for aggregating these individual errors could be using 

a. the sum of the residuals (SR),
b. the sum of absolute value of residuals (SAR), or
c. the sum of squared residuals (SSR)

Among these alternatives, (a) is not a useful aggregation as the positive residuals and negative residuals will cancel each other and (a) may misrepresent the total amount of error for all observations. Both (b) and (c) are plausible alternatives and can be used. On the other hand, (b) is less desirable because the absolute values are mathematically more difficult to deal with (ask a calculus professor!). So, (c) seems to be a good way of aggregating the total amount of error, it is mathematically easy to work with. We can show (c) in a mathematical notation as the following.

$$SSR = \sum_{i=1}^{N}(Y_i - (\beta_0+\beta_1X_i))^2$$
$$SSR = \sum_{i=1}^{N}(Y_i - \hat{Y_i})^2$$
$$SSR = \sum_{i=1}^{N}\epsilon_i^2$$
For our model, the sum of squared residuals would be 15.384.

```{r, echo=TRUE,eval=TRUE,class.source='klippy',class.source = 'fold-show',message=FALSE, warning=FALSE,fig.width=8,fig.height=8}

sum(d$error^2)
 
```

Now, how do we know that the set of coefficients we guesstimate ,{$\beta_0,\beta_1$} = {7.5,-2}, is a good model? Is there any other set of coefficients that would provide less error than this model? The only way of knowing this is to try a bunch of different models and see if we can find a better one that gives us better predictions (smaller residuals). But, there is literally infinite pairs of {$\beta_0,\beta_1$} coefficients, so which ones we should try?

Below, I will do a quick exploration. For instance, suppose the potential range for my intercept ($\beta_0$) is from -10 to 10 and I will consider every single possible value from -10 t 10 with increments of .1. Also, suppose the potential range for my slope ($\beta_1$) is from -2 to 2 and I will consider every single possible value from -2 to 2 with increments of .01. Given that every single combination of $\beta_0$ and $\beta_1$ indicates a different model, these settings suggest a total of 80,601 models to explore. If you are crazy enough, you can try every single model and compute the SSR. Then, we can plot them in a 3D by putting $\beta_0$ on the X-axis, $\beta_1$ on the Y-axis, and SSR on the Z-axis. Check the plot below and tell me if you can explore and find the minimum of this surface.

```{r, echo=FALSE,eval=FALSE,class.source='klippy',class.source = 'fold-show',message=FALSE, warning=FALSE,fig.width=8,fig.height=8}

grid    <- expand.grid(b0=seq(-10,10,.1),b1=seq(-5,5,.01))           
grid$SSR <- NA

for(i in 1:nrow(grid)){
  predicted    <- readability_sub$mean.wl*grid[i,]$b1 + grid[i,]$b0
  grid[i,]$SSR <- sum((readability_sub$target - predicted)^2)
  print(i)
}

  
  write.csv(grid, 
            here('data/grid_regression.csv'),
            row.names = FALSE)

```

```{r, echo=FALSE,eval=TRUE,class.source='klippy',class.source = 'fold-show',message=FALSE, warning=FALSE,fig.width=8,fig.height=8}

grid <- read.csv('https://raw.githubusercontent.com/uo-datasci-specialization/c4-ml-fall-2021/main/data/grid_regression.csv',header=TRUE)

plot_ly(grid, x = ~b0, y = ~b1, z = ~SSR, 
        marker = list(color = ~SSR,
                      showscale = TRUE,
                      cmin=min(grid$SSR)*5,
                      cmax=min(grid$SSR),cauto=F),
        width=600,height=600) %>% 
  add_markers()


```


The finding the best set of {$\beta_0,\beta_1$} coefficients that minimizes the sum of squared residuals is indeed an optimization problem. For any optimization problem, there is a **loss function** we either try to minimize or maximize. In this case, our loss function is the sum of squared residuals.

$$Loss = \sum_{i=1}^{N}(Y_i - (\beta_0+\beta_1X_i))^2$$
In this loss function, $X$ and $Y$ values are observed data, and {$\beta_0,\beta_1$} are unknown parameters. The goal of optimization is to find the set {$\beta_0,\beta_1$} coefficients that provides the minimum value of this function. Once this minima of this function is found, we can argue that the corresponding coefficients are our best solution for the regression model. 

In this case, this is a good-looking surface with a single global minima, and it is not difficult to find the minimum of this loss function. We also have an analytical solution to find its minima because of its simplicity. Most of the time, the optimization problems are more difficult, and we solve them using numerical techniques such as steepest ascent (or descent), newton-raphson, quasi-newton, genetic algorithm and many more. 

### Matrix Solution

For most regression problems, we can find the best set of coefficients with a simple matrix operations. Let's first see how we can represent the regression problem in matrix form. Suppose that I wrote the regression model presented in the earlier section for every single observation in a dataset with a sample size of N.


$$Y_1 = \beta_0  + \beta_1X_{1} + \epsilon_1.$$

$$Y_2 = \beta_0  + \beta_1X_{2} + \epsilon_2.$$
$$Y_3 = \beta_0  + \beta_1X_{3} + \epsilon_3.$$
$$...$$
$$...$$
$$...$$
$$Y_{20} = \beta_0  + \beta_1X_{20} + \epsilon_20.$$
We can write all of these equations in a much simpler format as

$$ \mathbf{Y} = \mathbf{X} \boldsymbol{\beta} + \boldsymbol{\epsilon}, $$
such that $\mathbf{Y}$ is an N x 1 column vector of observed values for the outcome variable, $\mathbf{X}$ is an N x (P+1) design matrix of observed values for predictor variables, and $\boldsymbol{\beta}$ is an (P+1) x 1 column vector of regression coefficients, and  $\boldsymbol{\epsilon}$ is an N x 1 column vector of residuals. For the problem above with our small dataset, these matrix elements would look like the following.

```{r, echo=FALSE,eval=knitr::is_html_output(),class.source='klippy',fig.align='center',fig.height=8,fig.width=8}

knitr::include_graphics(here('static/notes/images/regression_matrix.gif'))

```

```{r, echo=FALSE,eval=knitr::is_latex_output(),class.source='klippy',fig.align='center',fig.height=8,fig.width=8}

knitr::include_graphics(here('static/notes/images/regression_matrix.png'))

```

Or, more specifically, we can replace the observed values of $\mathbf{X}$  and $\mathbf{Y}$ with the corresponding elements. 

```{r, echo=FALSE,eval=knitr::is_html_output(),class.source='klippy',fig.align='center',fig.height=8,fig.width=8}

knitr::include_graphics(here('static/notes/images/data_regression_matrix.gif'))

```

```{r, echo=FALSE,eval=knitr::is_latex_output(),class.source='klippy',fig.align='center',fig.height=8,fig.width=8}

knitr::include_graphics(here('static/notes/images/data_regression_matrix.png'))

```

It can be shown that the set of {$\beta_0,\beta_1$} coefficients that yields the minimum sum of squared residuals for this model can be analytically found using the following matrix operation.

$$\hat{\boldsymbol{\beta}} = (\mathbf{X^T}\mathbf{X})^{-1}\mathbf{X^T}\mathbf{Y}$$

If we apply this matrix operation to our small datasets, we will find that the best set of  {$\beta_0,\beta_1$} coefficients to predict the readability score with the least amount of error using the average word length as a predictor is {$\beta_0,\beta_1$} = {4.494,-1.290}. These estimates are also known as the **least square estimates**, and the best linear unbiased estimators (BLUE) for the given regression model.

```{r, echo=TRUE,eval=TRUE,class.source='klippy',class.source = 'fold-show',message=FALSE, warning=FALSE,fig.width=8,fig.height=8}

Y <-  as.matrix(readability_sub$target)
X <-  as.matrix(cbind(1,readability_sub$mean.wl))

Y
X

beta <- solve(t(X)%*%X)%*%t(X)%*%Y

beta 
```

Once we find the best estimates for the model coefficients, we can also calculate the model predicted values and residual sum of squares for the given model and dataset.

$$\boldsymbol{\hat{Y}} = \mathbf{X} \hat{\boldsymbol{\beta}} $$

$$ \boldsymbol{\hat{\epsilon}} = \boldsymbol{Y} - \hat{\boldsymbol{Y}} $$
$$ RSS = \boldsymbol{\hat{\epsilon}^T} \boldsymbol{\hat{\epsilon}} $$

```{r, echo=TRUE,eval=TRUE,class.source='klippy',class.source = 'fold-show',message=FALSE, warning=FALSE,fig.width=8,fig.height=8}

Y_hat <-  X%*%beta

Y_hat

E <- Y - Y_hat
E

RSS <- t(E)%*%E

RSS
```

Note that the matrix formulation is generalized to a regression model for more than one predictor. When there are more predictors in the model, the dimensions of the design matrix ($\mathbf{X}$) and regression coefficient matrix ($\boldsymbol{\beta}$) will be different, but the matrix calculations will be identical. It is difficult to visualize the surface we are trying to minimize beyond two coefficients, but we know that the matrix solution will always provide us the set of coefficients that yields the least amount of error in our predictions.

Let's assume that we would like to expand our model by adding the number of sentences as the second predictor. Our new model will be

$$Y_i = \beta_0  + \beta_1X_{1i} + \beta_2X_{2i} + \epsilon_i.$$
Note that I added a subscript for $X$ to differentiate different predictors. Let's say $X_1$ represents the mean word length and $X_2$ represents the total number of sentence length. Now, we are looking for the best set of three coefficients, {$\beta_0,\beta_1, \beta_2$} that would yield the least amount of error in predicting the readability. Now, our matrix elements will look like the following:

```{r, echo=TRUE,eval=TRUE,class.source='klippy',class.source = 'fold-show',message=FALSE, warning=FALSE,fig.width=8,fig.height=8}

Y <-  as.matrix(readability_sub$target)
X <-  as.matrix(cbind(1,readability_sub[,c('mean.wl','sents')]))

Y
X
```

We will get the following estimates for {$\beta_0,\beta_1, \beta_2$} = {1.821, -.929, .090} yielding a value of 7.365 for the residual sum of squares.

```{r, echo=TRUE,eval=TRUE,class.source='klippy',class.source = 'fold-show',message=FALSE, warning=FALSE,fig.width=8,fig.height=8}

beta <- solve(t(X)%*%X)%*%t(X)%*%Y

beta 

Y_hat <-  X%*%beta

E <- Y - Y_hat

RSS <- t(E)%*%E

RSS
```

### Hat Matrix

**Hat matrix** plays an important role in diagnosing observations in the dataset with unusually high influence on predictions, and can be calculated in matrix form from the design matrix  ($\mathbf{X}$). In practice, it can be used to detect observations with potentially extreme values in terms of the predictors.

Remember that we did use the following matrix operation to calculate the predicted values,

$$\boldsymbol{\hat{Y}} = \mathbf{X} \hat{\boldsymbol{\beta}}.$$
We also know $\hat{\boldsymbol{\beta}}$ is equal,

$$\hat{\boldsymbol{\beta}} = (\mathbf{X^T}\mathbf{X})^{-1}\mathbf{X^T}\mathbf{Y}.$$
If we replace the matrix form of $\hat{\boldsymbol{\beta}}$ in the first equation, we obtain the following equation,

$$\boldsymbol{\hat{Y}} = \mathbf{X} (\mathbf{X^T}\mathbf{X})^{-1}\mathbf{X^T}\mathbf{Y},$$
or equivalently we can write

$$\boldsymbol{\hat{Y}} = \mathbf{H} \mathbf{Y},$$
where $\mathbf{H}$ is defined as 


$$\mathbf{H}= \mathbf{X}(\mathbf{X^T}\mathbf{X})^{-1}\mathbf{X^T}.$$



### `lm()` function



## Model Interpretation

## Model Evaluation

# Linear Regression with Regularization

## Ridge Penalty

## Lasso Penalty

## Elastic Net

# Wrapping up 

## Building a Prediction Model for Readability Scores

## Using the Prediction Model for a New Text





































