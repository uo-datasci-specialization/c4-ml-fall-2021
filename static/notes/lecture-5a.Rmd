---
title: Logistic Regression and Regularization
subtitle: Applied Machine Learning for Educational Data Science
author:
  name: Cengiz Zopluoglu
  affiliation: University of Oregon | EDLD 654
date: 10/26/2021 ## Or "Lecture no."
output: 
  html_document:
    keep_md: false
    theme: journal
    highlight: haddock
    code_folding: hide
    toc: yes
    toc_depth: 4
    toc_float: yes
  pdf_document:
    extra_dependencies: ["amssymb","animate","upgreek","amsmath"]
    keep_tex: false ## Change to true if want keep intermediate .tex file
    toc: true
    toc_depth: 3
    dev: cairo_pdf
always_allow_html: true
urlcolor: blue
mainfont: cochineal
sansfont: Fira Sans
monofont: Fira Code ## Although, see: https://tex.stackexchange.com/q/294362

## Automatically knit to both formats:
knit: (function(inputFile, encoding) {
 rmarkdown::render(inputFile, encoding = encoding, 
 output_format = 'all') 
 })
---

```{r klippy, echo=FALSE, include=TRUE}
klippy::klippy(position=c('top','right'))
```

`r paste('[Updated:',format(Sys.time(),'%a, %b %d, %Y - %H:%M:%S'),']')`

<style>
  
  .list-group-item.active, .list-group-item.active:focus, .list-group-item.active:hover {
    z-index: 2;
    color: #fff;
      background-color: #FC4445;
      border-color: #97CAEF;
  }

#infobox {
padding: 1em 1em 1em 4em;
margin-bottom: 10px;
border: 2px solid black;
border-radius: 10px;
background: #E6F6DC 5px center/3em no-repeat;
  }

</style>
  
  
```{r setup, include=FALSE}
knitr::opts_chunk$set(comment = "",fig.align='center')
require(here)
require(ggplot2)
require(plot3D)
require(kableExtra)
require(knitr)
require(giski)
require(magick)
require(gridExtra)
require(plotly)
options(scipen=99)


# Resources:

# https://bradleyboehmke.github.io/HOML/linear-regression.html
# https://bradleyboehmke.github.io/HOML/regularized-regression.html
# James et al. Ch 3 and Ch 6.2
# Applied Predictive Modeling, Chapter 6
```

# 1. Overview of the Logistic Regression

Logistic regression is a type of model that can be used to predict a binary outcome variable. Linear regression and logistic regression are indeed members of the same family of models called *generalized linear models*. While linear regression can also technically be used to predict a binary outcome, the bounded nature of a binary outcome, [0,1], makes the linear regression solution suboptimal. Logistic regression is a more appropriate model and takes the bounded nature of the binary outcome into account when making predictions.

The binary outcomes can be coded in a variety of ways in the data such as 0 vs 1, True vs False, Yes vs. No, Success vs. Failure. The rest of the notes, it is assumed that the category of interest to predict is represented by 1s in the data. 

The notes in this section will first introduce a suboptimal solution to predict a binary outcome by fitting a linear probability model using linear regression and discuss the limitations of this approach. Then, the logistic regression model and its estimation will be demonstrated. Finally, different regularization approaches for the logistic regression will be discussed.

Throughout these notes, we will use the [Recidivism dataset from the NIJ competition](https://nij.ojp.gov/funding/recidivism-forecasting-challenge) to discuss different aspects of logistic regression and demonstrations. This data and variables in this data were discussed in detail in [Lecture 1a](https://ml-21.netlify.app/notes/lecture-1a.html) and [Lecture 2a](https://ml-21.netlify.app/notes/lecture-2a.html). The outcome of interest to predict in this dataset is whether or not an individual will be recidivated in the second year after initial release. In order to make demonstrations easier, I randomly sample 20 observations from this data. Six observations in this data have a value of 1 for the outcome (recidivated) while 14 observations have a value of 0 (not recidivated).

```{r, echo=TRUE,eval=TRUE,class.source='klippy',class.source = 'fold-show',message=FALSE, warning=FALSE}

# Download the random sample of 20 observations from the recidivism dataset

recidivism_sub <- read.csv('https://raw.githubusercontent.com/uo-datasci-specialization/c4-ml-fall-2021/main/data/recidivism_sub.csv',
                         header=TRUE)

# Outcome variable

table(recidivism_sub$Recidivism_Arrest_Year2)
```  

## 1.1. Linear Probability Model

Linear probability model is just fitting a typical regression model to a binary outcome. When the outcome is binary, the predictions from a linear regression model can be considered as probability of outcome being equal to 1,
$$\hat{Y} = P(Y = 1).$$
Suppose that we want to predict the recidivism in the second year (`Recidivism_Arrest_Year2`) by using the number of dependents they have. Then, we could fit this using the `lm` function.

```{r, echo=TRUE,eval=TRUE,class.source='klippy',class.source = 'fold-show',message=FALSE, warning=FALSE}

mod <- lm(Recidivism_Arrest_Year2 ~ 1 + Dependents,
          data = recidivism_sub)

summary(mod)

```  

The intercept is 0.402 and the slope for the predictor `Dependents` is -.073. We can interpret the intercept and slope as the following for this example. Note that the predicted values from this model now can be interpreted as probability predictions because the outcome is binary.

- Intercept (0.402): When the number of dependent is equal to 0, the probability of being recidivated in Year 2 is 0.402.

- Slope (-0.073): For every additional dependent (one unit increase in X) the individual has, the probability of being recidivated in Year 2 is reduced by .07.

The intercept and slope still represent the best fitting line to our data, and this fitted line can be shown here.

```{r, echo=FALSE,eval=TRUE,class.source='klippy',class.source = 'fold-show',message=FALSE, warning=FALSE}

set.seed(1234)

x <- recidivism_sub$Dependents + runif(20,-.1,.1)
y <- recidivism_sub$Recidivism_Arrest_Year2

ggplot()+
  geom_point(aes(x=x,y=y))+
  theme_bw()+
  geom_abline(intercept = coef(mod)[1],slope = coef(mod)[2])+
  xlim(c(min(x),10))


```  


Now, suppose we want to calculate the model predicted probability of being recidivated in Year 2 for different number of dependents a parolee has. Let's assume that the number of dependents can be any number from 0 to 10. What would be the predicted probability of being recidivated in Year 2 for a parolee with 8 dependents?

```{r, echo=TRUE,eval=TRUE,class.source='klippy',class.source = 'fold-show',message=FALSE, warning=FALSE}

X <- data.frame(Dependents = 0:10)

predict(mod,newdata = X)

```  

It is not reasonable for a probability prediction to be negative. One of the major issues with using a linear regression to predict a binary outcome using a linear-probability model is that the model predictions can easily be go outside of the boundary [0,1] and yield unreasonable predictions. So, a linear regression model may not necessarily the best tool to predict a binary outcome. We should use a model that respect the boundaries of the outcome variable.  

## 1.2. Description of Logistic Regression Model

In order to overcome the limitations of the linear probability model, we bundle our prediction model in a sigmoid function. Suppose there is real-valued function of $a$ such that 

$$ f(a) = \frac{e^a}{e^a+1}. $$
It can be shows than the output of this function is always bounded to be between 0 and 1 regardless of the value of $a$. Therefore, sigmoid function is an appropriate choice for the logistic regression because it assures that the output is always bounded between 0 and 1. 

```{r, echo=FALSE,eval=TRUE,class.source='klippy',class.source = 'fold-show',message=FALSE, warning=FALSE}


fx <- function(x) { exp(x)/(1+exp(x))}

ggplot()+
  geom_function(fun=fx)+
  xlim(c(-10,10))+
  ylim(c(0,1))+
  theme_bw()+
  xlab('a')+
  ylab('f(a)')+
  ggtitle('Sigmoid function')


```  


If we revisit the previous example, we can specify a logistic regression model to predict the probability of being recidivated in Year 2 as the following by using the number of dependents a parolee has as the predictor,

$$P(Y=1) =  \frac{e^{\beta_0+\beta_1X}}{e^{\beta_0+\beta_1X}+1}.$$

When the values of predictor variable is entered into the equation, the model output can be directly interpreted as the probability of the binary outcome being equal to 1 (or whatever category and meaning a value of one represents). Then, we assume that the actual outcome follows a binomial distribution with the predicted probability.

$$ P(Y=1) = p $$
$$ Y \sim Binomial(p)$$

Suppose the coefficient estimates of this model are $\beta_0$=-0.38 and $\beta_1$=-0.37. Then, for instance, we can compute the probability of being recidivated for a parolee with 8 dependents as the following:

$$P(Y=1) =  \frac{e^{(-0.38-0.37 \times 8)}}{e^{(-0.38-0.37 \times 8)+1}} = 0.034.$$

```{r, echo=FALSE,eval=FALSE,class.source='klippy',class.source = 'fold-show',message=FALSE, warning=FALSE}

mod <- glm(Recidivism_Arrest_Year2 ~ 1 + Dependents,
           family = 'binomial',
           data = recidivism_sub)

summary(mod)

``` 

```{r, echo=TRUE,eval=TRUE,class.source='klippy',class.source = 'fold-show',message=FALSE, warning=FALSE}

b0 = -0.38
b1 = -0.37

x = 0:10

y = exp(b0+b1*x)/(1+exp(b0+b1*x))

data.frame(number.of.dependents = x, probability=y)
``` 

```{r, echo=FALSE,eval=TRUE,class.source='klippy',class.source = 'fold-show',message=FALSE, warning=FALSE}


b0 = -0.38
b1 = -0.37

fx <- function(x) {exp(b0+b1*x)/(1+exp(b0+b1*x))}

ggplot()+
  geom_function(fun=fx)+
  xlim(c(0,10))+
  ylim(c(0,1))+
  theme_bw()+
  xlab('Number of Dependents')+
  ylab('Probability of Being Recidivated')+
  ggtitle('')


```  

In its original form, it is difficult to interpret the parameters of the logistic regression because a one unit increase in the predictor is not anymore linearly related the probability of outcome being equal to 1 due to the nonlinear nature of the sigmoid function. Most common presentation of logistic regression is obtained after a bit of algebraic manipulation to rewrite the model equation. The logistic regression model above can also be specificed as the following without any loss of meaning as they are mathematically equivalent. 


$$ln \left [ \frac{P(Y=1)}{1-P(Y=1)} \right] =  \beta_0+\beta_1X.$$
The term on the left side of the equation is known as the **logit**. So, when the outcome is a binary variable, the logit transformation of the probability that the outcome is equal to 1 can be represented as a linear equation. This provides a more straightforward interpretation. For instance, we can know say that when the number of dependents is equal to zero, the predicted logit is equal to -0.38 (intercept), and for every additional dependent the logit decreases by 0.37 (slope). 

It is also common to transform the logit to odds when interpreting the parameters. For instance, we can say that when the number of dependents is equal to zero, the odds of being recidivated is 0.68 ($e^{-0.38}$), and for every additional dependent the odds of being recidivated is reduced by 31% ($1 - e^{-0.37}$).

The right side of the equation can be expanded by adding more predictors, adding polynomial terms of the predictors, or adding interactions among predictors. A model with only main effects of $P$ predictors can be written as

$$ ln \left [ \frac{P(Y=1)}{1-P(Y=1)} \right] = \beta_0  + \sum_{p=1}^{P} \beta_pX_{p},$$
and the coefficients can be interpreted as

- $\beta_0$: the predicted logit when the values for all the predictor variables in the model are equal to zero. $e^{\beta_0}$, the predicted odds of outcome being equal to 1 when the values for all the predictor variables in the model are equal to zero.

-  $\beta_p$: the change in the predicted logit for one unit increase in $X_p$ when the values for all other predictors in the model are held constant. For every one unit in increase in $X_p$, the odds of the outcome being equal to 1 is multiplied  by $e^{\beta_p}$ when the values for all other predictors in the model are held constant. In other words, $e^{\beta_p}$ is odds ratio, the ratio of odds at $\beta_p = a+1$ to the odds at $\beta_p = a$. 

It is important that you get familiar with the three concepts (probability, odds, logit) and how these three are related to each other for interpreting the logistic regression parameters. 

```{r, echo=FALSE,eval=knitr::is_html_output(),class.source='klippy',fig.align='center',fig.height=5,fig.width=12}

knitr::include_graphics(here('static/notes/images/pr-odd-logit.PNG'))

```

***
<div id="infobox">

<center style="color:black;"> **NOTE** </center>

Sigmoid function is not the only tool to be used for modeling a binary outcome. One can also use the cumulative standard normal distribution function, $\phi(a)$, and the output of $\phi(a)$ is also bounded between 0 and 1. When $\phi$ is used to to transform the prediction model, this is known as **probit regression** and it serves the same purpose as the logistic regression, which is to predict probability of a binary outcome being equal to 1. However, it is always easier and more pleasant to work with logarithmic functions, and logarithmic functions has nice computational properties. Therefore, logistic regression is more commonly used than the probit regression.
</div>
***

## 1.3. Model Estimation

### 1.3.1. The concept of likelihood

It is important to understand the concept of likelihood for estimating the coefficients of a logistic regression model. We will consider a simple example of flipping coins for this.
Suppose you flip the same coin 20 times and observe the following outcome. We don't necessarily know whether or not this is a fair coin in which the probability of observing a head or tail is equal to 0.5. 

$$\mathbf{Y} = \left ( H,H,H,T,H,H,H,T,H,T \right )$$
Suppose that we define $p$ as the probability of observing a head when we flip this coin. By definition, the probability of observing a tail is $1-p$.
$$P(Y=H) = p$$
$$P(Y=T) = 1 - p$$
Then, we can calculate the likelihood of our observations of heads and tails as a function of $p$.

$$ \mathfrak{L}(\mathbf{Y}|p) = p \times p \times p \times (1-p) \times p \times p \times p \times (1-p) \times p \times (1-p) $$
For instance, if we say that this is a fair coin and therefore $p$ is equal to 0.5, then the likelihood of observing 7 heads and 3 tails would be equal to

$$ \mathfrak{L}(\mathbf{Y}|p = 0.5) = 0.5 \times 0.5 \times 0.5 \times (1-0.5) \times 0.5 \times 0.5 \times 0.5 \times (1-0.5) \times 0.5 \times (1-0.5) = 0.0009765625$$

On the other hand, another person can say this is probably not a fair coin and the $p$ should be something higher than 0.5. How about 0.65?


$$ \mathfrak{L}(\mathbf{Y}|p = 0.65) = 0.65 \times 0.5 \times 0.65 \times (1-0.65) \times 0.65 \times 0.65 \times 0.65 \times (1-0.65) \times 0.65 \times (1-0.65) = 0.00210183$$

We can say that based on our observation, an estimate of $p$ being equal to 0.65 is more likely than an estimate of $p$ being equal to 0.5. Our observation of 7 heads and 3 tails is more likely if we estimate $p$ as 0.65 rather than 0.5.

### 1.3.2. Maximum likelihood estimation (MLE)

Then, what would be the best estimate of $p$ given our observed data (7 heads and 3 tails). We can try every possible value of $p$ between 0 and 1, and calculate the likelihood of our data ($\mathbf{Y}$). Then, we can pick the value that makes our data most likely (largest likelihood) to observe as our best estimate. This would be called the maximum likelihood estimate of $p$ given the data we observed.

```{r, echo=TRUE,eval=TRUE,class.source='klippy',class.source = 'fold-show',message=FALSE, warning=FALSE}

p <- seq(0,1,.001)
L <- p^7*(1-p)^3

ggplot()+
  geom_line(aes(x=p,y=L)) + 
  theme_bw() + 
  xlab('Probability of Observing a Head (p)')+
  ylab('Likelihood of Observing 7 Heads and 3 Tails')+
  geom_vline(xintercept=p[which.max(L)],lty=2)
```

We can show that the $p$ value that makes the likelihood largest is 0.7, and the likelihood of observing 7 heads and 3 tails is 0.002223566 when $p$ is equal to 0.7. Therefore, the maximum likelihood estimate of probability of observing a head for this particular coin is 0.7 given the 10 observations we have made. 

```{r, echo=TRUE,eval=TRUE,class.source='klippy',class.source = 'fold-show',message=FALSE, warning=FALSE}

L[which.max(L)]

p[which.max(L)]
``` 

Note that our estimate can change and be updated if we continue collecting more data by flipping the same coin and record our observations.

### 1.3.3. The concept of loglikelihood

The computation of likelihood requires the multiplication of so many $p$ values, and When you multiply values between 0 and 1, the result gets smaller and smaller. This creates problems when you multiply so many of these small $p$ values due the maximum precision any computer can handle. For instance, you can see what is the minimum number that can be represented in R and meets the requirements of [IEEE 754 technical standard](https://en.wikipedia.org/wiki/IEEE_754).

```{r, echo=TRUE,eval=TRUE,class.source='klippy',class.source = 'fold-show',message=FALSE, warning=FALSE}

.Machine$double.xmin

```

When you have hundreds of thousands of observations, it is probably not a good idea to directly work with likelihood. Instead, we work with the log of likelihood. This has two main advantages:

- We are less concerned about the precision of small numbers our computer can handle.

- Loglikelihood has nicer mathematical properties to work with for optimization problems (log of product of two numbers is equal to the sum of log of the two numbers).

- The point that maximizes likelihood also the same number that maximizes the loglikelihood, so our end results (MLE estimate) does not care if we use loglikelihood instead of likelihood.

```{r, echo=TRUE,eval=TRUE,class.source='klippy',class.source = 'fold-show',message=FALSE, warning=FALSE}

p <- seq(0,1,.001)
logL <- log(p)*7 + log(1-p)*3

ggplot()+
  geom_line(aes(x=p,y=logL)) + 
  theme_bw() + 
  xlab('Probability of Observing a Head (p)')+
  ylab('Loglikelihood of Observing 7 Heads and 3 Tails')+
  geom_vline(xintercept=p[which.max(logL)],lty=2)
```

```{r, echo=TRUE,eval=TRUE,class.source='klippy',class.source = 'fold-show',message=FALSE, warning=FALSE}

logL[which.max(logL)]

p[which.max(logL)]
``` 

### 1.3.4. MLE for Logistic Regression coefficients

Now, we can apply these concepts to estimate the logistic regression coefficients. Let's revisit our previous example in which we predict the probability of being recidivated in Year 2 given the number of dependents a parolee has. Our model can be written as the following.

$$ln \left [ \frac{P_i(Y=1)}{1-P_i(Y=1)} \right] =  \beta_0+\beta_1X_i.$$
Note that $X$ and $P$ has a subscript $i$ to indicate that each individual may have a different X value, and therefore each individual will have a different probability. Our observed outcome is a set of 0s and 1s. Remember that there are 6 individuals recidivated (Y=1) and 14 individuals not recidivated (Y=0).

```{r, echo=TRUE,eval=TRUE,class.source='klippy',class.source = 'fold-show',message=FALSE, warning=FALSE}

recidivism_sub$Recidivism_Arrest_Year2
``` 

Given a set of coefficients, {$\beta_0,\beta_1$}, we can calculate the logit for every observation using the model equation, and then transform this logit to a probability,$P_i(Y=1)$. Finally, we can calculate the log of the probability for each obsrvation, and sum them across observations to obtain the loglikelihood of observing this set of observations (14 ones and 6 zeros). Suppose that we have two guesstimates for {$\beta_0,\beta_1$}, and they are 0.5 and -0.8, respectively. These coefficients imply the following predicted model.

```{r, echo=FALSE,eval=TRUE,class.source='klippy',class.source = 'fold-show',message=FALSE, warning=FALSE}


b0 = 0.5
b1 = -0.8

fx <- function(x) {exp(b0+b1*x)/(1+exp(b0+b1*x))}

ggplot()+
  geom_function(fun=fx)+
  xlim(c(0,10))+
  ylim(c(0,1))+
  theme_bw()+
  xlab('Number of Dependents')+
  ylab('Probability of Being Recidivated')+
  ggtitle('')


```  

If these two coefficients are our estimates, how likely would it be to observe the outcome in our data given the number of dependents. The below R code first finds the predicted logit for every single observation assuming that $\beta_0$ = 0.5 and $\beta_1$ = -0.8.


```{r, echo=TRUE,eval=TRUE,class.source='klippy',class.source = 'fold-show',message=FALSE, warning=FALSE}

b0 = 0.5
b1 = -0.8

x = recidivism_sub$Dependents
y = recidivism_sub$Recidivism_Arrest_Year2

pred_logit <- b0 + b1*x

pred_prob1 <- exp(pred_logit)/(1+exp(pred_logit))
pred_prob0 <- 1 - pred_prob1 

data.frame(Dependents      = x, 
           Recidivated     = y, 
           Prob1 = pred_prob1,
           Prob0 = pred_prob0)

``` 

```{r, echo=TRUE,eval=TRUE,class.source='klippy',class.source = 'fold-show',message=FALSE, warning=FALSE}

logL <-  y*log(pred_prob1) + (1-y)*log(pred_prob0)

sum(logL)

``` 

We can sumamrize this by saying that if our model coefficients are $\beta_0$ = 0.5 and $\beta_1$ = -0.8, then the log of likelihood of observing the outcome in our data would be -12.65.

$$\mathbf{Y} = \left ( 1,0,1,0,0,0,0,1,1,0,0,1,0,0,0,1,0,0,0,0 \right )$$


$$ \mathfrak{logL}(\mathbf{Y}|\beta_0 = 0.5,\beta_1 = -0.8) = -12.65$$
The critical question to ask is whether or not there is another pair of values we can assign to $\beta_0$ and $\beta_1$ that would provide a higher likelihood of data. If there is, then they would be better estimates for our model. If we can find such a pair with the maximum loglikelihood of data, then they would be our maximum likelihood estimates for the given model. 

We can approach this problem in a very crude way to gain some intuition about what Maximum Likelihood Estimation is about. Now, suppose that a reasonable range of values for  $\beta_0$ is from -1 to 1 and a reasonable range of values for  $\beta_1$ is also from -1 to 1. Let's think about every possible combinations of values for $\beta_0$ and  $\beta_1$ within these ranges with increments of .01. Then, let's calculate the loglikelihood of data for every possible combination and plot these in a 3D plot as a function of $\beta_0$ and  $\beta_1$.

```{r, echo=TRUE,eval=FALSE,class.source='klippy',class.source = 'fold-show',message=FALSE, warning=FALSE,fig.width=8,fig.height=8}

grid    <- expand.grid(b0=seq(-1,1,.01),b1=seq(-1,1,.01))           
grid$logL <- NA

for(i in 1:nrow(grid)){
  
  x = recidivism_sub$Dependents
  y = recidivism_sub$Recidivism_Arrest_Year2

  pred_logit    <- grid[i,]$b0 + grid[i,]$b1*x
  pred_prob1    <- exp(pred_logit)/(1+exp(pred_logit))
  pred_prob0    <- 1 - pred_prob1 
  logL          <- y*log(pred_prob1) + (1-y)*log(pred_prob0)
  grid[i,]$logL <- sum(logL)
  
  print(i)
}

  
require(plotly)

plot_ly(grid, x = ~b0, y = ~b1, z = ~logL, 
        marker = list(color = ~logL,
                      showscale = FALSE,
                      cmin=min(grid$logL),
                      cmax=max(grid$logL),cauto=F),
        width=600,height=600) %>% 
  add_markers()

```

```{r, echo=FALSE,eval=TRUE,class.source='klippy',class.source = 'fold-show',message=FALSE, warning=FALSE,fig.width=8,fig.height=8}

grid <- read.csv('https://raw.githubusercontent.com/uo-datasci-specialization/c4-ml-fall-2021/main/data/grid_logistic_regression.csv',header=TRUE)

plot_ly(grid, x = ~b0, y = ~b1, z = ~logL, 
        marker = list(color = ~logL,
                      showscale = FALSE,
                      cmin=min(grid$logL),
                      cmax=max(grid$logL),cauto=F),
        width=600,height=600) %>% 
  add_markers()


```

What is the maximum point of this surface? Our crude search indicates that it is -11.78708, and the set of $\beta_0$ and $\beta_1$ coefficients that make the observed data most likely is -0.38 and -0.37. 

```{r, echo=TRUE,eval=TRUE,class.source='klippy',class.source = 'fold-show',message=FALSE, warning=FALSE}

grid[which.max(grid$logL),]

``` 

Therefore, given our dataset with 20 observations, our maximum likelihood estimates for the coefficients of the logistic regression model above are -0.38 and -0.37.


$$ln \left [ \frac{P_i(Y=1)}{1-P_i(Y=1)} \right] =  -0.38 - 0.37 \times X_i.$$

### 1.3.5. Logistic Loss function

Below is a compact way of writing likelihood and loglikeihood in mathematical notation. For simplification purposes, we write $P_i$ to represent $P_i(Y=1)$.

$$ \mathfrak{L}(\mathbf{Y}|\boldsymbol\beta) = \prod_{i=1}^{N} P_i^{y_i} \times (1-P_i)^{1-y_i}$$


$$ \mathfrak{logL}(\mathbf{Y}|\boldsymbol\beta) = \sum_{i=1}^{N} Y_i \times ln(P_i) + (1-Y_i) \times ln(1-P_i)$$

The final equation above, $\mathfrak{logL}(\mathbf{Y}|\boldsymbol\beta)$, is known as the **logistic loss** function. By finding the set of coefficients in a model, $\boldsymbol\beta = (\beta_0, \beta_1,...,\beta_P)$, that maximizes this quantity, we obtain the maximum likelihood estimates of the coefficients for the logistic regression model.

Unfortunately, the naive crude search we applied above would be a bad solution when you have a complex model with so many predictors. Another unfortunate thing is that there is no closed form solution (as we had for the linear regression) for the logistic regression. Therefore, the only way to estimate the logistic regression coefficients is to use numerical approximations and computational algorithms to maximize the logistic loss function. Luckily, we have tools available to accomplish this task.

***
<div id="infobox">

<center style="color:black;"> **NOTE** </center>

Why do we not use least square estimation and minimize the sum of squared residuals when estimating the coefficients of the logistic regression model? We can certainly use the sum of squared residuals as our loss function and minimize it to estimate the coefficients for the logistic regression, just like we did for the linear regression. The complication is that the sum of squared residuals function yields a non-convex surface when the outcome is binary as opposed to a convex surface obtained from the logistic loss function. Non-convex optimization problems are harder than convex optimization problems, and they are more vulnerable in terms of finding sub-optimal solutions (local minima/maxima). Therefore, the logistic loss function and maximizing it is preferred when estimating the coefficients of a logistic regression model.
 
```{r, echo=FALSE,eval=FALSE,class.source='klippy',class.source = 'fold-show',message=FALSE, warning=FALSE,fig.width=8,fig.height=8}


#If you are interested, I replicated the previous crude grid search by using the sum of #squared residuals as the loss function. As you see, the least-square estimates are very #close to the maximum-likelihood estimates. This is not surprising since this is a simple #model with only one predictor.

grid    <- expand.grid(b0=seq(-1,1,.01),b1=seq(-1,1,.01))           
grid$SSR <- NA

for(i in 1:nrow(grid)){
  
  x = recidivism_sub$Dependents
  y = recidivism_sub$Recidivism_Arrest_Year2

  pred_logit    <- grid[i,]$b0 + grid[i,]$b1*x
  pred_prob1    <- exp(pred_logit)/(1+exp(pred_logit))
 
  grid[i,]$SSR  <- sum((y - pred_prob1)^2)
  
  print(i)
}

  
require(plotly)

plot_ly(grid, x = ~b0, y = ~b1, z = ~SSR, 
        marker = list(color = ~SSR,
                      showscale = FALSE,
                      cmin=min(grid$SSR),
                      cmax=max(grid$SSR),cauto=F),
        width=600,height=600) %>% 
  add_markers()

```

```{r, echo=FALSE,eval=FALSE,class.source='klippy',class.source = 'fold-show',message=FALSE, warning=FALSE,fig.width=8,fig.height=8}

grid <- read.csv('https://raw.githubusercontent.com/uo-datasci-specialization/c4-ml-fall-2021/main/data/grid_logistic_regression_ssr.csv',header=TRUE)

plot_ly(grid, x = ~b0, y = ~b1, z = ~SSR, 
        marker = list(color = ~SSR,
                      showscale = FALSE,
                      cmin=min(grid$SSR),
                      cmax=max(grid$SSR),cauto=F),
        width=600,height=600) %>% 
  add_markers()

```

```{r, echo=FALSE,eval=FALSE,class.source='klippy',class.source = 'fold-show',message=FALSE, warning=FALSE}

grid[which.min(grid$SSR),]

``` 

</div>
***

### 1.3.6. The `glm` function

The `glm()` function as a part of base `stats` package can be used to estimate the coefficients of the logistic regression. Let's fit the model discussed in the earlier sections using the `glm()` function.

The use of the `glm()` function is very similar to the `lm()` function. The only difference is that we specify the `family='binomial'` argument to fit the logistic regression by maximizing the logistic loss function.


```{r, echo=TRUE,eval=TRUE,class.source='klippy',class.source = 'fold-show',message=FALSE, warning=FALSE,fig.width=8,fig.height=8}

mod <- glm(Recidivism_Arrest_Year2 ~ 1 + Dependents,
           data   = recidivism_sub,
           family = 'binomial')

summary(mod)

```

In the **Coefficients** table, the numbers under the **Estimate** column are the estimated coefficients for the logistic regression model. The quantity labeled as the **Residual Deviance** in the output is twice the maximized loglikelihood,

$$ Deviance  = -2 \times \mathfrak{logL}(\mathbf{Y}|\boldsymbol\beta). $$
Notice that the coefficient estimates from the `glm()` function are very close to our crude estimates from a brute-force search in an earlier section (-0.38 and -0.37). From our crude search, we found the maximum loglikelihood as -11.787. So, if we multiply that number by -2, that is equal to 23.574, which is the number reported in this output as **Residual Deviance**.


### 1.3.7. The `glmnet` function

You can also `glmnet()` function from the `glmnet` package to fit the logistic regression. The advantage of the `glmnet` package is that it provides the option of regularization while fitting the logistic regression. You can set the `alpha=0` and `lambda=0` arguments to obtain the coefficient estimates without any penalty.

```{r, echo=TRUE,eval=TRUE,class.source='klippy',class.source = 'fold-show',message=FALSE, warning=FALSE,fig.width=8,fig.height=8}

require(glmnet)

mod <- glmnet(x         = cbind(0,recidivism_sub$Dependents),
              y         = factor(recidivism_sub$Recidivism_Arrest_Year2),
              family    = 'binomial',
              alpha     = 0,
              lambda    = 0,
              intercept = TRUE)

coef(mod)

```

The `x` argument is the input matrix for predictors, and `y` argument is a vector of binary response outcome. The `glmnet` requires the `y` argument to be a factor with two levels. Note that I defined `x` argument above as `cbind(0,recidivism_sub$Dependents)` because `glmnet` requires the `x` to be a matrix with at least two columns. So, I added a column of zeros to trick the function and force to run it. That column of zeros has zero impact on the estimation.

## 1.4. Building a Prediction Model for Recidivism

In earlier weeks, we discussed how to process 48 variables in the recidivism data and constructed 165 features for a given individual using a blueprint. Let's read the original data.

```{r, echo=TRUE,eval=TRUE,class.source='klippy',class.source = 'fold-show',message=FALSE, warning=FALSE}

recidivism <- read.csv('https://raw.githubusercontent.com/uo-datasci-specialization/c4-ml-fall-2021/main/data/recidivism_y1 removed.csv',header=TRUE)
```

This dataset has 18,111 rows and 54 columns. Each row represents a parolee. There are 48 predictor variables, one ID variable, four possible outcome variable (Recidivism in Year 1, Recidivism in Year 2, Recidivism in Year 3, and Recidivism within 3 years), and a filter variable to obtain the train and test split as in the original competition. For this lecture, we are interested in predicting Recidivism in Year 2. This dataset has already cleaned by removing the individuals who were recidivated in Year 1 ([Lecture-1a](https://ml-21.netlify.app/notes/lecture-1a.html#Recidivism)) 

```{r, echo=TRUE,eval=TRUE,class.source='klippy',class.source = 'fold-show',message=FALSE, warning=FALSE}

table(recidivism$Recidivism_Arrest_Year1)

table(recidivism$Recidivism_Arrest_Year2)

table(recidivism$Recidivism_Arrest_Year3)

table(recidivism$Training_Sample)

```

### 1.4.1. Initial data preparation

We will copy/paste the blueprint we developed earlier in the course ([Lecture 2a](https://ml-21.netlify.app/notes/lecture-2a.html#6_Wrapping-up_using_the_recipes_package))


```{r, echo=TRUE,eval=TRUE,class.source='klippy',class.source = 'fold-show',message=FALSE, warning=FALSE}

# List of variable types
  
  outcome <- c('Recidivism_Arrest_Year2')
  
  categorical <- c('Residence_PUMA',
                   'Prison_Offense',
                   'Age_at_Release',
                   'Supervision_Level_First',
                   'Education_Level',
                   'Prison_Years',
                   'Gender',
                   'Race',
                   'Gang_Affiliated',
                   'Prior_Arrest_Episodes_DVCharges',
                   'Prior_Arrest_Episodes_GunCharges',
                   'Prior_Conviction_Episodes_Viol',
                   'Prior_Conviction_Episodes_PPViolationCharges',
                   'Prior_Conviction_Episodes_DomesticViolenceCharges',
                   'Prior_Conviction_Episodes_GunCharges',
                   'Prior_Revocations_Parole',
                   'Prior_Revocations_Probation',
                   'Condition_MH_SA',
                   'Condition_Cog_Ed',
                   'Condition_Other',
                   'Violations_ElectronicMonitoring',
                   'Violations_Instruction',
                   'Violations_FailToReport',
                   'Violations_MoveWithoutPermission',
                   'Employment_Exempt') 

  numeric   <- c('Supervision_Risk_Score_First',
                 'Dependents',
                 'Prior_Arrest_Episodes_Felony',
                 'Prior_Arrest_Episodes_Misd',
                 'Prior_Arrest_Episodes_Violent',
                 'Prior_Arrest_Episodes_Property',
                 'Prior_Arrest_Episodes_Drug',
                 'Prior_Arrest_Episodes_PPViolationCharges',
                 'Prior_Conviction_Episodes_Felony',
                 'Prior_Conviction_Episodes_Misd',
                 'Prior_Conviction_Episodes_Prop',
                 'Prior_Conviction_Episodes_Drug',
                 'Delinquency_Reports',
                 'Program_Attendances',
                 'Program_UnexcusedAbsences',
                 'Residence_Changes',
                 'Avg_Days_per_DrugTest',
                 'Jobs_Per_Year')
  
  props      <- c('DrugTests_THC_Positive',
                  'DrugTests_Cocaine_Positive',
                  'DrugTests_Meth_Positive',
                  'DrugTests_Other_Positive',
                  'Percent_Days_Employed')
  
# 3) Convert all nominal, ordinal, and binary variables to factors
  # Leave the rest as is
  
  for(i in categorical){
    
    recidivism[,i] <- as.factor(recidivism[,i])
    
  }
  
# 4) For variables that represent proportions, add/substract a small number
  # to 0s/1s for logit transformation
  
  for(i in props){
    recidivism[,i] <- ifelse(recidivism[,i]==0,.0001,recidivism[,i])
    recidivism[,i] <- ifelse(recidivism[,i]==1,.9999,recidivism[,i])
  }
  

#############################################################################
  
# Blueprint for processing variables
  
require(recipes)

blueprint <- recipe(x  = recidivism,
                    vars  = c(categorical,numeric,props,outcome),
                    roles = c(rep('predictor',48),'outcome')) %>%
  step_indicate_na(all_of(categorical),all_of(numeric),all_of(props)) %>%
  step_zv(all_numeric()) %>%
  step_impute_mean(all_of(numeric),all_of(props)) %>%
  step_impute_mode(all_of(categorical)) %>%
  step_logit(all_of(props)) %>%
  step_ns(all_of(numeric),all_of(props),deg_free=3) %>%
  step_normalize(paste0(numeric,'_ns_1'),
                 paste0(numeric,'_ns_2'),
                 paste0(numeric,'_ns_3'),
                 paste0(props,'_ns_1'),
                 paste0(props,'_ns_2'),
                 paste0(props,'_ns_3')) %>%
  step_dummy(all_of(categorical),one_hot=TRUE) %>%
  step_num2factor(Recidivism_Arrest_Year2,
                  transform = function(x) x + 1,
                  levels=c('No','Yes'))

  # The final step_num2factor is necessary to be able to work with the 
  # caret::train() function
  # This syntax simply converts 0s and 1s to 1s and 2s, then convert it to a factor
  # with labels attached (No:1, Yes:2)

blueprint
```

### 1.4.2. Train/Test split

We could do a random split of training and test sample. However, this dataset has already a filtering variable created by the agency who opened this competition. The test sample was unknown during the competition, and was later released after the competition was over. I will use the original training and testing datasets so we can compare the performance of our models to the highest ranking results in the competition.

```{r, echo=TRUE,eval=TRUE,class.source='klippy',class.source = 'fold-show',message=FALSE, warning=FALSE}

loc <- which(recidivism$Training_Sample==1)

# Training dataset

recidivism_tr  <- recidivism[loc, ]
dim(recidivism_tr)

# Test dataset

recidivism_te  <- recidivism[-loc, ]
dim(recidivism_te)
```

### 1.4.3. Model fitting with the `caret` package

`caret` package has `glm` available as an engine to use.

```{r, echo=TRUE,eval=TRUE,class.source='klippy',class.source = 'fold-show',message=FALSE, warning=FALSE,fig.width=8,fig.height=8}


require(caret)

# getModelInfo()$glm

```

For 10-fold cross-validation, I will first do a random shuffle of the data and then assign each individual to a fold. Note that I added two arguments, `summaryFunction = mnLogLoss` and `classProbs=TRUE` for the `trainControl()` function. This is necessary to fit the logistic regression by minimizing the negative logistic loss function (or maximizing the logistic loss). 

***
<div id="infobox">

<center style="color:black;"> **NOTE** </center>

`mnLogLoss` yields computation of **minus log-likelihood**. The negative of loglikelihood is also sometimes called **the cross-entropy loss function**. Technically, maximizing the logistic loss function is equivalent to minimizing the cross-entropy loss function. When you use `glmnet` and specify `summaryFunction = mnLogLoss` in the cross-validation settings, it is minimizing the negative of the logistic loss function. You should be aware of it because it may be confusing or misleading when you start tuning parameters based on the value of logloss reported by the `glmnet` function. When you are using `caret::train()` for training a `glmnet` model with a binary outcome, keep in mind that a lower value o logloss function is better because the function actually minimizes the negative of the logloss function.

</div>
***


```{r, echo=TRUE,eval=TRUE,class.source='klippy',class.source = 'fold-show',message=FALSE, warning=FALSE,fig.width=8,fig.height=8}

# Cross validation settings

  # Create the index values for 10-folds to provide to the 
  # trainControl function. This way, you can reproduce the results in the future
  # and use the same folds across models.

    # Randomly shuffle the data

      set.seed(10302021) # for reproducibility

      recidivism_tr = recidivism_tr[sample(nrow(recidivism_tr)),]

    # Create 10 folds with equal size

      folds = cut(seq(1,nrow(recidivism_tr)),breaks=10,labels=FALSE)
  
    # Create the list for each fold 
      
      my.indices <- vector('list',10)
      for(i in 1:10){
        my.indices[[i]] <- which(folds!=i)
      }
      

      cv <- trainControl(method    = "cv",
                   index           = my.indices,
                   classProbs      = TRUE,
                   summaryFunction = mnLogLoss)
      
      
```

Now, we can train the logistic regression model using 10-fold cross validation.

```{r, echo=TRUE,eval=FALSE,class.source='klippy',class.source = 'fold-show',message=FALSE, warning=FALSE,fig.width=8,fig.height=8}

# Train the model
  
caret_mod <- caret::train(blueprint, 
                          data      = recidivism_tr, 
                          method    = "glm",
                          family    = 'binomial',
                          metric    = 'logLoss',
                          trControl = cv)

caret_mod
```

```{r, echo=FALSE,eval=TRUE,class.source='klippy',class.source = 'fold-show',message=FALSE, warning=FALSE,fig.width=8,fig.height=8}

load("B:/UO Teaching/EDLD 654/Books&Resources/largedata/logistic_regression_model_lecture5a.RData")

```

After we train the model, we can predict the probabilities for the observation in the test dataset.

```{r, echo=TRUE,eval=TRUE,class.source='klippy',class.source = 'fold-show',message=FALSE, warning=FALSE,fig.width=8,fig.height=8}

# Predict the probabilities for the observations in the test dataset

predicted_te <- predict(caret_mod, recidivism_te, type='prob')

dim(predicted_te)

head(predicted_te)

```

Finally, we can calculate several evaluation metrics (e.g., AUC, TPR, TNR, FPR, Precision) for the model.

```{r, echo=TRUE,eval=TRUE,class.source='klippy',class.source = 'fold-show',message=FALSE, warning=FALSE,fig.width=8,fig.height=8}

# Compute the AUC

require(cutpointr)

cut.obj <- cutpointr(x     = predicted_te$Yes,
                     class = recidivism_te$Recidivism_Arrest_Year2)

auc(cut.obj)

# Confusion matrix assuming the threshold is 0.5

pred_class <- ifelse(predicted_te$Yes>.5,1,0)

confusion <- table(recidivism_te$Recidivism_Arrest_Year2,pred_class)

confusion

# True Negative Rate

confusion[1,1]/(confusion[1,1]+confusion[1,2])


# False Positive Rate

confusion[1,2]/(confusion[1,1]+confusion[1,2])

# True Positive Rate

confusion[2,2]/(confusion[2,1]+confusion[2,2])
 

# Precision

confusion[2,2]/(confusion[1,2]+confusion[2,2])
 
```

# 2. Regularization in Logistic Regression

The regularization works very similarly in logistic regression as it was discussed in linear regression. We add penalty terms to the loss function to avoid large coefficients. By including a penalty term, we reduce model variance in exchange of adding bias. By optimizing the degree of penalty via tuning, we can typically get models with better performance than a logistic regression with no regularization.

## 2.1 Ridge Penalty

The loss function with ridge penalty applied in logistic regression is the following:

$$ \mathfrak{logL}(\mathbf{Y}|\boldsymbol\beta) = \left ( \sum_{i=1}^{N} Y_i \times ln(P_i) + (1-Y_i) \times ln(1-P_i) \right ) - \frac{\lambda}{2} \sum_{i=1}^{P}\beta_p^2$$
Notice that we now substract the penalty from the logistic loss, because we are maximizing the quantity. The penalty term has the same effect in the logistic regression, and it will pull the regression coefficients toward zero, but will not make them exactly equal to zero. Below, you can see a plot of change in logistic regression coefficients for some of the variables from a model that will be fit in the next section at increasing levels of ridge penalty.

```{r, echo=FALSE,eval=TRUE,class.source='klippy',class.source = 'fold-show',message=FALSE, warning=FALSE,fig.width=8,fig.height=8}

coef_ridge <- read.csv(here('data','ridge coefs.csv')) 

ggplot()+
  geom_line(aes(x=seq(0,9.9,.1),y=as.numeric(coef_ridge[7,])))+
  geom_line(aes(x=seq(0,9.9,.1),y=as.numeric(coef_ridge[3,])))+
  geom_line(aes(x=seq(0,9.9,.1),y=as.numeric(coef_ridge[112,])))+
  theme_bw()+
  xlab('Lambda')+
  ylab('Logistic regression coefficients')


```


***
<div id="infobox">

<center style="color:black;"> **NOTE** </center>

The `glmnet` package divides the value of loss function by sample size ($N$) during the optimization (Equation 12 and 2-3 in [this paper](https://www.jstatsoft.org/article/view/v033i01)). This technically does not change anything in terms of the optimal solution. On the other hand, we should be aware that the ridge penalty being applied in the `glmnet package has become

$$N\frac{\lambda}{2}\sum_{i=1}^{P}\beta_p^2.$$

This information may be important while searching plausible values of $\lambda$ for the `glmnet` package.

</div>
***

### 2.1.1. Model Fitting with the `caret` package

```{r, echo=TRUE,eval=FALSE,class.source='klippy',class.source = 'fold-show',message=FALSE, warning=FALSE,fig.width=8,fig.height=8}

# Cross-validation settings

cv <- trainControl(method    = "cv",
                   index           = my.indices,
                   classProbs      = TRUE,
                   summaryFunction = mnLogLoss)
      
# Hyperparameter tuning grid for ridge penalty (lambda), alpha = 0

grid <- data.frame(alpha = 0, lambda = c(seq(0,.001,.00001),.005,.01,.05,.1)) 
grid

# Train the model
  
caret_logistic_ridge <- caret::train(blueprint, 
                                     data      = recidivism_tr, 
                                     method    = "glmnet",
                                     family    = 'binomial',
                                     metric    = 'logLoss',
                                     trControl = cv,
                                     tuneGrid  = grid)

caret_logistic_ridge

# check the results

plot(caret_logistic_ridge)

caret_logistic_ridge$bestTune

```


```{r, echo=FALSE,eval=TRUE,class.source='klippy',class.source = 'fold-show',message=FALSE, warning=FALSE,fig.width=8,fig.height=8}

load("B:/UO Teaching/EDLD 654/Books&Resources/largedata/logistic_regression_ridgemodel_lecture5a.RData")

plot(caret_logistic_ridge)

caret_logistic_ridge$bestTune

```


```{r, echo=TRUE,eval=TRUE,class.source='klippy',class.source = 'fold-show',message=FALSE, warning=FALSE,fig.width=8,fig.height=8}

# Predict the probabilities for the observations in the test dataset

predicted_te <- predict(caret_logistic_ridge, recidivism_te, type='prob')

dim(predicted_te)

head(predicted_te)

# Compute the AUC

require(cutpointr)

cut.obj <- cutpointr(x     = predicted_te$Yes,
                     class = recidivism_te$Recidivism_Arrest_Year2)

auc(cut.obj)

# Confusion matrix assuming the threshold is 0.5

pred_class <- ifelse(predicted_te$Yes>.5,1,0)

confusion <- table(recidivism_te$Recidivism_Arrest_Year2,pred_class)

confusion

# True Negative Rate

confusion[1,1]/(confusion[1,1]+confusion[1,2])

# False Positive Rate

confusion[1,2]/(confusion[1,1]+confusion[1,2])

# True Positive Rate

confusion[2,2]/(confusion[2,1]+confusion[2,2])
 

# Precision

confusion[2,2]/(confusion[1,2]+confusion[2,2])
 
```
             
### 2.1.2. Variable Importance

```{r, echo=TRUE,eval=TRUE,class.source='klippy',class.source = 'fold-show',message=FALSE, warning=FALSE,fig.width=8,fig.height=6}

#install.packages('vip')

require(vip)

vip(caret_logistic_ridge, 
    num_features = 10, 
    geom = "point") + 
  theme_bw()



coefs <- coef(caret_logistic_ridge$finalModel,caret_logistic_ridge$bestTune$lambda)

ind   <- order(abs(coefs[,1]),decreasing=T)

head(as.matrix(coefs[ind,]),10)

```

## 2.2. Lasso Penalty

The loss function with the lasso penalty applied in logistic regression is the following:

$$ \mathfrak{logL}(\mathbf{Y}|\boldsymbol\beta) = \left ( \sum_{i=1}^{N} Y_i \times ln(P_i) + (1-Y_i) \times ln(1-P_i) \right)- \lambda \sum_{i=1}^{P}|\beta_p|$$
The penalty terms has the same effect in the logistic regression, and it will make the regression coefficients equal to zero when a sufficiently large $\lambda$ value is applied.

Below, you can see a plot of change in logistic regression coefficients for some of the variables from a model that will be fit in the next section at increasing levels of lasso penalty.

```{r, echo=FALSE,eval=TRUE,class.source='klippy',class.source = 'fold-show',message=FALSE, warning=FALSE,fig.width=8,fig.height=8}

coef_lasso <- read.csv(here('data','lasso coefs.csv')) 

ggplot()+
  geom_line(aes(x=seq(0,0.07,.001),y=as.numeric(coef_lasso[7,])))+
  geom_line(aes(x=seq(0,0.07,.001),y=as.numeric(coef_lasso[3,])))+
  geom_line(aes(x=seq(0,0.07,.001),y=as.numeric(coef_lasso[112,])))+
  theme_bw()+
  xlab('Lambda')+
  ylab('Logistic regression coefficients')


```

***
<div id="infobox">

<center style="color:black;"> **NOTE** </center>

Note that `glmnet` package divides the value of loss function by sample sample ($N$) during the optimization (Equation 12 and 2-3 in [this paper](https://www.jstatsoft.org/article/view/v033i01)). You should be aware that the lasso penalty being applied in the `glmnet package has become

$$N\lambda\sum_{i=1}^{P}|\beta_p|.$$

This information may be important while searching plausible values of $\lambda$ for the `glmnet` package.

</div>
***

### 2.2.1. Model Fitting with the `caret` package

```{r, echo=TRUE,eval=FALSE,class.source='klippy',class.source = 'fold-show',message=FALSE, warning=FALSE,fig.width=8,fig.height=8}

# Cross-validation settings

cv <- trainControl(method    = "cv",
                   index           = my.indices,
                   classProbs      = TRUE,
                   summaryFunction = mnLogLoss)
      
# Hyperparameter tuning grid for ridge penalty (lambda), alpha = 0

grid <- data.frame(alpha = 1, lambda = seq(0,.001,.00001)) 
grid

  # If you want to fit logistic regression you can also set lambda to 0
  # grid <- data.frame(alpha = 0, lambda = 0) 
  # This is equivalent to glm fit we did in the previous section

# Train the model
  
caret_logistic_lasso <- caret::train(blueprint, 
                                     data      = recidivism_tr, 
                                     method    = "glmnet",
                                     family    = 'binomial',
                                     metric    = 'logLoss',
                                     trControl = cv,
                                     tuneGrid  = grid)

caret_logistic_lasso

# check the results

plot(caret_logistic_lasso)

caret_logistic_lasso$bestTune

```


```{r, echo=FALSE,eval=TRUE,class.source='klippy',class.source = 'fold-show',message=FALSE, warning=FALSE,fig.width=8,fig.height=8}

load("B:/UO Teaching/EDLD 654/Books&Resources/largedata/logistic_regression_lassomodel_lecture5a.RData")

plot(caret_logistic_lasso)

caret_logistic_lasso$bestTune

```


```{r, echo=TRUE,eval=TRUE,class.source='klippy',class.source = 'fold-show',message=FALSE, warning=FALSE,fig.width=8,fig.height=8}

# Predict the probabilities for the observations in the test dataset

predicted_te <- predict(caret_logistic_lasso, recidivism_te, type='prob')

dim(predicted_te)

head(predicted_te)

# Compute the AUC

require(cutpointr)

cut.obj <- cutpointr(x     = predicted_te$Yes,
                     class = recidivism_te$Recidivism_Arrest_Year2)

auc(cut.obj)

# Confusion matrix assuming the threshold is 0.5

pred_class <- ifelse(predicted_te$Yes>.5,1,0)

confusion <- table(recidivism_te$Recidivism_Arrest_Year2,pred_class)

confusion

# True Negative Rate

confusion[1,1]/(confusion[1,1]+confusion[1,2])

# False Positive Rate

confusion[1,2]/(confusion[1,1]+confusion[1,2])

# True Positive Rate

confusion[2,2]/(confusion[2,1]+confusion[2,2])
 

# Precision

confusion[2,2]/(confusion[1,2]+confusion[2,2])
 
```

### 2.2.2. Variable Importance

```{r, echo=TRUE,eval=TRUE,class.source='klippy',class.source = 'fold-show',message=FALSE, warning=FALSE,fig.width=8,fig.height=6}

#install.packages('vip')

require(vip)

vip(caret_logistic_lasso, 
    num_features = 10, 
    geom = "point") + 
  theme_bw()



coefs <- coef(caret_logistic_lasso$finalModel,
              caret_logistic_lasso$bestTune$lambda)

ind   <- order(abs(coefs[,1]),decreasing=T)

head(as.matrix(coefs[ind,]),10)

```

## 2.3. Elastic Net


The loss function with the elastic applied is the following:

$$ \mathfrak{logL}(\mathbf{Y}|\boldsymbol\beta) = \left ( \sum_{i=1}^{N} Y_i \times ln(P_i) + (1-Y_i) \times ln(1-P_i) \right)- \left ((1-\alpha)\frac{\lambda}{2} \sum_{i=1}^{P}\beta_p^2 + \alpha  \lambda \sum_{i=1}^{P}|\beta_p| \right)$$

### 2.3.1. Model Fitting with the `caret` package

```{r, echo=TRUE,eval=FALSE,class.source='klippy',class.source = 'fold-show',message=FALSE, warning=FALSE,fig.width=8,fig.height=8}

# Cross-validation settings

cv <- trainControl(method    = "cv",
                   index           = my.indices,
                   classProbs      = TRUE,
                   summaryFunction = mnLogLoss)
      
# Hyperparameter tuning grid for ridge penalty (lambda), alpha = 0

  grid <- expand.grid(alpha = seq(0,.8,.01), lambda = seq(0,.001,.0001)) 
  grid
  
# Train the model
  
caret_logistic_elastic <- caret::train(blueprint, 
                                     data      = recidivism_tr, 
                                     method    = "glmnet",
                                     family    = 'binomial',
                                     metric    = 'logLoss',
                                     trControl = cv,
                                     tuneGrid  = grid)

caret_logistic_elastic

# check the results

plot(caret_logistic_elastic)

caret_logistic_elastic$bestTune


0.5091198
```


```{r, echo=FALSE,eval=TRUE,class.source='klippy',class.source = 'fold-show',message=FALSE, warning=FALSE,fig.width=8,fig.height=8}

load("B:/UO Teaching/EDLD 654/Books&Resources/largedata/logistic_regression_elastic_lecture5a.RData")


plot(caret_logistic_elastic)

caret_logistic_elastic$bestTune


```


```{r, echo=TRUE,eval=TRUE,class.source='klippy',class.source = 'fold-show',message=FALSE, warning=FALSE,fig.width=8,fig.height=8}

# Predict the probabilities for the observations in the test dataset

predicted_te <- predict(caret_logistic_elastic, recidivism_te, type='prob')

dim(predicted_te)

head(predicted_te)

# Compute the AUC

require(cutpointr)

cut.obj <- cutpointr(x     = predicted_te$Yes,
                     class = recidivism_te$Recidivism_Arrest_Year2)

auc(cut.obj)

# Confusion matrix assuming the threshold is 0.5

pred_class <- ifelse(predicted_te$Yes>.5,1,0)

confusion <- table(recidivism_te$Recidivism_Arrest_Year2,pred_class)

confusion

# True Negative Rate

confusion[1,1]/(confusion[1,1]+confusion[1,2])

# False Positive Rate

confusion[1,2]/(confusion[1,1]+confusion[1,2])

# True Positive Rate

confusion[2,2]/(confusion[2,1]+confusion[2,2])
 

# Precision

confusion[2,2]/(confusion[1,2]+confusion[2,2])
 
```

### 2.3.2. Variable Importance

```{r, echo=TRUE,eval=TRUE,class.source='klippy',class.source = 'fold-show',message=FALSE, warning=FALSE,fig.width=8,fig.height=6}

#install.packages('vip')

require(vip)

vip(caret_logistic_elastic, 
    num_features = 10, 
    geom = "point") + 
  theme_bw()



coefs <- coef(caret_logistic_elastic$finalModel,
              caret_logistic_elastic$bestTune$lambda)

ind   <- order(abs(coefs[,1]),decreasing=T)

head(as.matrix(coefs[ind,]),10)

```


## 2.4. Comparison of Models

Performance evaluation metrics on the test data.

|                                         | -LL  | AUC  | ACC | TPR | TNR | FPR |PRE  |
|-----------------------------------------|:----:|:----:|:---:|:---:|:---:|:---:|:---:|
| Logistic Regression                     |0.5096|0.7192|0.755|0.142|0.949|0.051|0.471|
| Logistic Regression with Ridge Penalty  |0.5111|0.7181|0.754|0.123|0.954|0.046|0.461|
| Logistic Regression with Lasso Penalty  |0.5090|0.7200|0.754|0.127|0.952|0.048|0.458|
| Logistic Regression with Elastic Net    |0.5091|0.7200|0.753|0.127|0.952|0.048|0.456|
























