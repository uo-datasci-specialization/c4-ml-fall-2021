---
title: Performance Evaluation Metrics
subtitle: Applied Machine Learning for Educational Data Science
author:
  name: Cengiz Zopluoglu
  affiliation: University of Oregon | EDLD 654
date: 09/15/2021 ## Or "Lecture no."
output: 
  html_document:
    keep_md: false
    theme: journal
    highlight: haddock
    code_folding: hide
    toc: yes
    toc_depth: 4
    toc_float: yes
  pdf_document:
    extra_dependencies: ["amssymb","animate"]
    keep_tex: false ## Change to true if want keep intermediate .tex file
    toc: true
    toc_depth: 3
    dev: cairo_pdf
always_allow_html: true
urlcolor: blue
mainfont: cochineal
sansfont: Fira Sans
monofont: Fira Code ## Although, see: https://tex.stackexchange.com/q/294362

## Automatically knit to both formats:
knit: (function(inputFile, encoding) {
 rmarkdown::render(inputFile, encoding = encoding, 
 output_format = 'all') 
 })
---



```{r klippy, echo=FALSE, include=TRUE}
klippy::klippy(position=c('top','right'))
```

`r paste('[Updated:',format(Sys.time(),'%a, %b %d, %Y - %H:%M:%S'),']')`

<style>

.list-group-item.active, .list-group-item.active:focus, .list-group-item.active:hover {
    z-index: 2;
    color: #fff;
    background-color: #FC4445;
    border-color: #97CAEF;
}

#infobox {
  padding: 1em 1em 1em 4em;
  margin-bottom: 10px;
  border: 2px solid black;
  border-radius: 10px;
  background: #E6F6DC 5px center/3em no-repeat;
}

</style>


```{r setup, include=FALSE}
knitr::opts_chunk$set(comment = "",fig.align='center')
require(here)
require(ggplot2)
require(plot3D)
require(kableExtra)
require(knitr)
require(giski)
require(magick)
require(gridExtra)
options(scipen=99)

# Resources:

 # Kuhn and Johnson, Applied Predictive Modeling, Chapter 5.1, 11.1
 # https://scikit-learn.org/stable/modules/model_evaluation.html

```

In the next few weeks, we will talk about different approaches to build predictive models. We need some performance metrics to evaluate how useful a model is by itself, or how well it performs compared to alternative models. We will consider the performance metrics in two main categories: 1) metrics for predicting a continuous outcome, 2) metrics for predicting a binary outcome. 

We will use two hypothetical small datasets (N = 30) for introducing these concepts. The first hypothetical dataset will have a continuous outcome and predictions. The second hypothetical dataset will have a binary outcome and predictions.

You can load these two hypothetical datasets using the code below:

```{r, echo=TRUE,eval=FALSE,class.source='klippy',class.source = 'fold-show',message=FALSE, warning=FALSE}

readability <- read.csv('https://raw.githubusercontent.com/uo-datasci-specialization/c4-ml-fall-2021/main/data/readability.csv',
                        header=TRUE)
```



# 1. Evaluation Metrics for Continuous Outcomes

## Explained Variance ($R^2$)

## Error related indices

### Mean Absolute Error

### Median Absolute Error

### Mean Squared Error

### Root Mean Squared Error

### Squared Log Error

### Mean Absolute Percentage Error

## Baseline performance considerations


# 2. Evaluation Metrics for Binary Outcomes

## Brier Score

## Confusion Matrix

### Accuracy

### True Positive Rate 

### False Positive Rate

### Precision

### Recall

## F1 score

## AUC

## Log loss

## Custome metrics for profit/cost optimization

## Baseline performance considerations

## Baseline performance






















































